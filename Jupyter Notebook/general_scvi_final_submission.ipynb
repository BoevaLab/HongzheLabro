{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scvi\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def malignant_cell_collection(adata, malignant_cell_incices, label_key):\n",
    "    mdata = adata[adata.obs[label_key].isin(adata.obs[label_key].unique()[[i for i in malignant_cell_incices]])]\n",
    "    return mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_preprocessing(adata, batch_key):\n",
    "    \n",
    "    if \"counts\" not in adata.layers.keys():\n",
    "        adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "    if scipy.sparse.issparse(adata.layers['counts']):\n",
    "        if np.any([(k%1) for k in adata.layers['counts'].todense().ravel()]):\n",
    "            adata.layers['counts'] = np.round(adata.layers['counts'].todense())\n",
    "    else:\n",
    "        if np.any([(k%1) for k in adata.layers['counts'].ravel()]):\n",
    "            adata.layers['counts'] = np.round(adata.layers['counts'])\n",
    "            \n",
    "    sc.pp.normalize_total(adata, target_sum=10e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    adata.raw = adata\n",
    "    sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    n_top_genes=2000,\n",
    "    subset=True,\n",
    "    layer=\"counts\",\n",
    "    batch_key=batch_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(adata, layer_key, batch_key, n_layers, n_hidden, n_latent, lr):\n",
    "    adata = adata.copy()\n",
    "    scvi.model.SCVI.setup_anndata(adata, layer=layer_key, batch_key = batch_key)\n",
    "    model = scvi.model.SCVI(adata, n_layers = n_layers, n_hidden=n_hidden, n_latent=n_latent)\n",
    "    model.train(max_epochs=100, validation_size=0.1, check_val_every_n_epoch=5, early_stopping=True, \n",
    "    early_stopping_monitor='elbo_validation', early_stopping_patience = 20, plan_kwargs={'lr':lr})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction_loss_and_elbo(model):\n",
    "    train_recon_loss = model.history['reconstruction_loss_train']\n",
    "    elbo_train = model.history['elbo_train']\n",
    "    elbo_val = model.history['elbo_validation']\n",
    "    val_recon_loss = model.history['reconstruction_loss_validation']\n",
    "    ax = train_recon_loss.plot()\n",
    "    elbo_train.plot(ax = ax)\n",
    "    elbo_val.plot(ax = ax)\n",
    "    val_recon_loss.plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_UMAP(model, adata, batch_key, label_key, added_latent_key, print_UMAP):\n",
    "    latent = model.get_latent_representation()\n",
    "    adata.obsm[added_latent_key] = latent\n",
    "    sc.pp.neighbors(adata, use_rep=added_latent_key, n_neighbors=20)\n",
    "    sc.tl.umap(adata, min_dist=0.3)\n",
    "    if print_UMAP:\n",
    "        sc.pl.umap(adata, color = [label_key, batch_key])\n",
    "    return added_latent_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def save_latent(adata: AnnData, latent_key: str, dataset_name: str) -> None:\n",
    "    if latent_key in adata.obsm_keys():\n",
    "        latent = pd.DataFrame(adata.obsm[latent_key], index=adata.obs_names)\n",
    "        latent.to_csv(f\"{dataset_name}_latent.csv\")\n",
    "\n",
    "def plot_integration(\n",
    "        adata: AnnData, dataset_name: str, batch_key: str, group_key: str\n",
    ") -> None:\n",
    "    sc.settings.figdir = \".\"\n",
    "    sc.tl.umap(adata)\n",
    "    sc.pl.umap(\n",
    "        adata, color=[batch_key, group_key], save=f\"_{dataset_name}.png\", show=False\n",
    "    )\n",
    "\n",
    "\n",
    "def get_partition(gpu: bool) -> str:\n",
    "    if gpu:\n",
    "        return \"gpu\"\n",
    "    return \"compute\"\n",
    "\n",
    "\n",
    "def get_gres(gpu: bool) -> Optional[str]:\n",
    "    if gpu:\n",
    "        return \"gpu:rtx2080ti:1\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def diffusion_nn(adata, k, max_iterations=26):\n",
    "    \"\"\"\n",
    "    Diffusion neighbourhood score\n",
    "    This function generates a nearest neighbour list from a connectivities matrix\n",
    "    as supplied by BBKNN or Conos. This allows us to select a consistent number\n",
    "    of nearest neighbours across all methods.\n",
    "    Return:\n",
    "       `k_indices` a numpy.ndarray of the indices of the k-nearest neighbors.\n",
    "    \"\"\"\n",
    "    if \"neighbors\" not in adata.uns:\n",
    "        raise ValueError(\n",
    "            \"`neighbors` not in adata object. \" \"Please compute a neighbourhood graph!\"\n",
    "        )\n",
    "\n",
    "    if \"connectivities\" not in adata.obsp:\n",
    "        raise ValueError(\n",
    "            \"`connectivities` not in `adata.obsp`. \"\n",
    "            \"Please pass an object with connectivities computed!\"\n",
    "        )\n",
    "\n",
    "    T = adata.obsp[\"connectivities\"]\n",
    "\n",
    "    # Row-normalize T\n",
    "    T = sparse.diags(1 / T.sum(1).A.ravel()) * T\n",
    "\n",
    "    T_agg = T ** 3\n",
    "    M = T + T ** 2 + T_agg\n",
    "    i = 4\n",
    "\n",
    "    while ((M > 0).sum(1).min() < (k + 1)) and (i < max_iterations):\n",
    "        # note: k+1 is used as diag is non-zero (self-loops)\n",
    "        print(f\"Adding diffusion to step {i}\")\n",
    "        T_agg *= T\n",
    "        M += T_agg\n",
    "        i += 1\n",
    "\n",
    "    if (M > 0).sum(1).min() < (k + 1):\n",
    "        raise ValueError(\n",
    "            f\"could not find {k} nearest neighbors in {max_iterations}\"\n",
    "            \"diffusion steps.\\n Please increase max_iterations or reduce\"\n",
    "            \" k.\\n\"\n",
    "        )\n",
    "\n",
    "    M.setdiag(0)\n",
    "    k_indices = np.argpartition(M.A, -k, axis=1)[:, -k:]\n",
    "\n",
    "    return k_indices\n",
    "\n",
    "\n",
    "def diffusion_conn(adata, min_k=50, copy=True, max_iterations=26):\n",
    "    \"\"\"\n",
    "    Diffusion for connectivites matrix extension\n",
    "    This function performs graph diffusion on the connectivities matrix until a\n",
    "    minimum number `min_k` of entries per row are non-zero.\n",
    "    Note:\n",
    "    Due to self-loops min_k-1 non-zero connectivies entries is actually the stopping\n",
    "    criterion. This is equivalent to `sc.pp.neighbors`.\n",
    "    Returns:\n",
    "       The diffusion-enhanced connectivities matrix of a copy of the AnnData object\n",
    "       with the diffusion-enhanced connectivities matrix is in\n",
    "       `adata.uns[\"neighbors\"][\"conectivities\"]`\n",
    "    \"\"\"\n",
    "    if \"neighbors\" not in adata.uns:\n",
    "        raise ValueError(\n",
    "            \"`neighbors` not in adata object. \" \"Please compute a neighbourhood graph!\"\n",
    "        )\n",
    "\n",
    "    if \"connectivities\" not in adata.obsp:\n",
    "        raise ValueError(\n",
    "            \"`connectivities` not in `adata.obsp`. \"\n",
    "            \"Please pass an object with connectivities computed!\"\n",
    "        )\n",
    "\n",
    "    T = adata.obsp[\"connectivities\"]\n",
    "\n",
    "    # Normalize T with max row sum\n",
    "    # Note: This keeps the matrix symmetric and ensures |M| doesn't keep growing\n",
    "    T = sparse.diags(1 / np.array([T.sum(1).max()] * T.shape[0])) * T\n",
    "\n",
    "    M = T\n",
    "\n",
    "    # Check for disconnected component\n",
    "    n_comp, labs = sparse.csgraph.connected_components(\n",
    "        adata.obsp[\"connectivities\"], connection=\"strong\"\n",
    "    )\n",
    "\n",
    "    if n_comp > 1:\n",
    "        tab = pd.value_counts(labs)\n",
    "        small_comps = tab.index[tab < min_k]\n",
    "        large_comp_mask = np.array(~pd.Series(labs).isin(small_comps))\n",
    "    else:\n",
    "        large_comp_mask = np.array([True] * M.shape[0])\n",
    "\n",
    "    T_agg = T\n",
    "    i = 2\n",
    "    while ((M[large_comp_mask, :][:, large_comp_mask] > 0).sum(1).min() < min_k) and (\n",
    "            i < max_iterations\n",
    "    ):\n",
    "        print(f\"Adding diffusion to step {i}\")\n",
    "        T_agg *= T\n",
    "        M += T_agg\n",
    "        i += 1\n",
    "\n",
    "    if (M[large_comp_mask, :][:, large_comp_mask] > 0).sum(1).min() < min_k:\n",
    "        raise ValueError(\n",
    "            \"could not create diffusion connectivities matrix\"\n",
    "            f\"with at least {min_k} non-zero entries in\"\n",
    "            f\"{max_iterations} iterations.\\n Please increase the\"\n",
    "            \"value of max_iterations or reduce k_min.\\n\"\n",
    "        )\n",
    "\n",
    "    M.setdiag(0)\n",
    "\n",
    "    if copy:\n",
    "        adata_tmp = adata.copy()\n",
    "        adata_tmp.uns[\"neighbors\"].update({\"diffusion_connectivities\": M})\n",
    "        return adata_tmp\n",
    "\n",
    "    else:\n",
    "        return M\n",
    "\n",
    "\n",
    "def split_batches(adata: AnnData, batch_key: str) -> List[AnnData]:\n",
    "    splits = []\n",
    "    for batch in adata.obs[batch_key].cat.categories:\n",
    "        splits.append(adata[adata.obs[batch_key] == batch].copy())\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal  # pytype: disable=not-supported-yet\n",
    "from typing import Optional\n",
    "\n",
    "import anndata as an  # pytype: disable=import-error\n",
    "import numpy as np\n",
    "import pydantic  # pytype: disable=import-error\n",
    "import scanpy as sc  # pytype: disable=import-error\n",
    "from anndata import AnnData\n",
    "\n",
    "_SupportedMetric = Literal[\n",
    "    \"cityblock\",\n",
    "    \"cosine\",\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"manhattan\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "    \"chebyshev\",\n",
    "    \"correlation\",\n",
    "    \"dice\",\n",
    "    \"hamming\",\n",
    "    \"jaccard\",\n",
    "    \"kulsinski\",\n",
    "    \"mahalanobis\",\n",
    "    \"minkowski\",\n",
    "    \"rogerstanimoto\",\n",
    "    \"russellrao\",\n",
    "    \"seuclidean\",\n",
    "    \"sokalmichener\",\n",
    "    \"sokalsneath\",\n",
    "    \"sqeuclidean\",\n",
    "    \"yule\",\n",
    "]\n",
    "\n",
    "\n",
    "class NeighborsGraphConfig(pydantic.BaseModel):\n",
    "    \"\"\"Settings for neighborhood graph computation.\n",
    "    For description, see\n",
    "    https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html\n",
    "    \"\"\"\n",
    "\n",
    "    n_neighbors: int = pydantic.Field(default=15)\n",
    "    n_pcs: Optional[int] = pydantic.Field(default=None)\n",
    "    knn: bool = pydantic.Field(default=True)\n",
    "    # TODO(Pawel): Check whether we can support other methods as well.\n",
    "    method: Literal[\"umap\"] = pydantic.Field(default=\"umap\")\n",
    "    metric: _SupportedMetric = pydantic.Field(default=\"euclidean\")\n",
    "\n",
    "\n",
    "class _LeidenBaseConfig(pydantic.BaseModel):\n",
    "    nngraph: NeighborsGraphConfig = pydantic.Field(default_factory=NeighborsGraphConfig)\n",
    "    random_state: int = pydantic.Field(default=0)\n",
    "    directed: bool = pydantic.Field(default=True)\n",
    "    use_weights: bool = pydantic.Field(default=True)\n",
    "    n_iterations: int = pydantic.Field(default=-1)\n",
    "\n",
    "\n",
    "class BinSearchSettings(pydantic.BaseModel):\n",
    "    start: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3, description=\"The minimal resolution.\"\n",
    "    )\n",
    "    end: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=10.0, description=\"The maximal resolution.\"\n",
    "    )\n",
    "    epsilon: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3,\n",
    "        description=\"Controls the maximal number of iterations before throwing lookup \"\n",
    "        \"error.\",\n",
    "    )\n",
    "\n",
    "    @pydantic.validator(\"end\")\n",
    "    def validate_end_greater_than_start(cls, v, values, **kwargs) -> float:\n",
    "        if v <= values[\"start\"]:\n",
    "            raise ValueError(\"In binary search end must be greater than start.\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class LeidenNClusterConfig(_LeidenBaseConfig):\n",
    "    clusters: int = pydantic.Field(\n",
    "        default=5, description=\"The number of clusters to be returned.\"\n",
    "    )\n",
    "    binsearch: BinSearchSettings = pydantic.Field(default_factory=BinSearchSettings)\n",
    "\n",
    "\n",
    "class LeidenNCluster:\n",
    "    def __init__(self, settings: LeidenNClusterConfig) -> None:\n",
    "        self._settings = settings\n",
    "\n",
    "    def fit_predict(self, adata: AnnData, key_added: str) -> np.ndarray:\n",
    "        for offset in [0, 20_000, 30_000, 40_000]:\n",
    "            points = _binary_search_leiden_resolution(\n",
    "                adata,\n",
    "                k=self._settings.clusters,\n",
    "                key_added=key_added,\n",
    "                random_state=self._settings.random_state + offset,\n",
    "                directed=self._settings.directed,\n",
    "                use_weights=self._settings.use_weights,\n",
    "                start=self._settings.binsearch.start,\n",
    "                end=self._settings.binsearch.end,\n",
    "                _epsilon=self._settings.binsearch.epsilon,\n",
    "            )\n",
    "            if points is not None:\n",
    "                break\n",
    "        # In case that for multiple random seeds we didn't find a resolution that\n",
    "        # matches the number of clusters, we raise a ValueError.\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No resolution for the number of clusters {self._settings.clusters}\"\n",
    "                f\" found.\"\n",
    "            )\n",
    "\n",
    "        return points.obs[key_added].astype(int).values\n",
    "\n",
    "\n",
    "def _binary_search_leiden_resolution(\n",
    "    adata: an.AnnData,\n",
    "    k: int,\n",
    "    start: float,\n",
    "    end: float,\n",
    "    key_added: str,\n",
    "    random_state: int,\n",
    "    directed: bool,\n",
    "    use_weights: bool,\n",
    "    _epsilon: float,\n",
    ") -> Optional[an.AnnData]:\n",
    "    \"\"\"Binary search to get the resolution corresponding\n",
    "    to the right k.\"\"\"\n",
    "    # We try the resolution which is in the middle of the interval\n",
    "    res = 0.5 * (start + end)\n",
    "\n",
    "    # Run Leiden clustering\n",
    "    sc.tl.leiden(\n",
    "        adata,\n",
    "        resolution=res,\n",
    "        key_added=key_added,\n",
    "        random_state=random_state,\n",
    "        directed=directed,\n",
    "        use_weights=use_weights,\n",
    "    )\n",
    "\n",
    "    # Get the number of clusters found\n",
    "    selected_k = adata.obs[key_added].nunique()\n",
    "    if selected_k == k:\n",
    "        return adata\n",
    "\n",
    "    # If the start and the end are too close (and there is no point in doing another\n",
    "    # iteration), we raise an error that one can't find the required number of clusters\n",
    "    if abs(end - start) < _epsilon * res:\n",
    "        return None\n",
    "\n",
    "    if selected_k > k:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=start,\n",
    "            end=res,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )\n",
    "    else:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=res,\n",
    "            end=end,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List, Optional, Union\n",
    "\n",
    "import bbknn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scanpy.external as sce\n",
    "import scvi\n",
    "from anndata import AnnData\n",
    "from cansig.integration.model import CanSig\n",
    "from omegaconf import MISSING\n",
    "\n",
    "#from utils import split_batches\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str = MISSING\n",
    "    gpu: bool = False\n",
    "    malignant_only: bool = True\n",
    "    batch_key: str = \"sample_id\"\n",
    "    latent_key: str = \"latent\"\n",
    "    n_top_genes: int = 2000\n",
    "\n",
    "\n",
    "\n",
    "def run_model(adata: AnnData, cfg) -> Tuple[AnnData, float]:\n",
    "    start = timer()\n",
    "    if cfg.name == \"bbknn\":\n",
    "        adata = run_bbknn(adata, config=cfg)\n",
    "    elif cfg.name == \"scvi\":\n",
    "        adata = run_scvi(adata, config=cfg)\n",
    "    elif cfg.name == \"scanorama\":\n",
    "        adata = run_scanorama(adata, config=cfg)\n",
    "    elif cfg.name == \"harmony\":\n",
    "        adata = run_harmony(adata, config=cfg)\n",
    "    elif cfg.name == \"cansig\":\n",
    "        adata = run_cansig(adata, config=cfg)\n",
    "    elif cfg.name == \"nmm\":\n",
    "        adata = run_mnn(adata, config=cfg)\n",
    "    elif cfg.name == \"combat\":\n",
    "        adata = run_combat(adata, config=cfg)\n",
    "    elif cfg.name == \"desc\":\n",
    "        adata = run_desc(adata, config=cfg)\n",
    "    elif cfg.name == \"dhaka\":\n",
    "        adata = run_dhaka(adata, config=cfg)\n",
    "    elif cfg.name == \"scanvi\":\n",
    "        adata = run_scanvi(adata, config=cfg)\n",
    "    elif cfg.name == \"trvaep\":\n",
    "        adata = run_trvaep(adata, config=cfg)\n",
    "    elif cfg.name == \"scgen\":\n",
    "        adata = run_scgen(adata, config=cfg)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{cfg.name} is not implemented.\")\n",
    "    run_time = timer() - start\n",
    "    return adata, run_time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DhakaConfig(ModelConfig):\n",
    "    name: str = \"dhaka\"\n",
    "    gpu: bool = True\n",
    "\n",
    "    n_latent: int = 3\n",
    "    # Data preprocessing\n",
    "    n_genes: int = 5000\n",
    "    total_expression: float = 1e6\n",
    "    pseudocounts: int = 1\n",
    "    # Training\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 50\n",
    "    learning_rate: float = 1e-4\n",
    "    clip_norm: float = 2.0\n",
    "    # Magic flag\n",
    "    scale_reconstruction_loss: bool = True\n",
    "\n",
    "\n",
    "def run_dhaka(adata: AnnData, config: DhakaConfig) -> AnnData:\n",
    "    import dhaka.api as dh\n",
    "\n",
    "    new_config = dh.DhakaConfig(\n",
    "        n_latent=config.n_latent,\n",
    "        n_genes=config.n_genes,\n",
    "        total_expression=config.total_expression,\n",
    "        pseudocounts=config.pseudocounts,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        learning_rate=config.learning_rate,\n",
    "        clip_norm=config.clip_norm,\n",
    "        scale_reconstruction_loss=config.scale_reconstruction_loss\n",
    "    )\n",
    "\n",
    "    return dh.run_dhaka(adata, config=new_config, key_added=config.latent_key)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScanVIConfig(ModelConfig):\n",
    "    name: str = \"scanvi\"\n",
    "    malignant_only: bool = False\n",
    "\n",
    "\n",
    "def run_scanvi(adata: AnnData, config: ScanVIConfig) -> AnnData:\n",
    "    raise NotImplementedError(\"This method requires several celltypes to run.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrVAEpConfig(ModelConfig):\n",
    "    name: str = \"trvaep\"\n",
    "    n_top_genes: int = 3000\n",
    "    n_latent: int = 10\n",
    "    alpha: float = 1e-4\n",
    "    layer1: int = 64\n",
    "    layer2: int = 32\n",
    "    seed: int = 42  # Random seed\n",
    "    # Training params\n",
    "    epochs: int = 300\n",
    "    batch_size: int = 1024\n",
    "    early_patience: int = 50\n",
    "    learning_rate: float = 1e-3\n",
    "\n",
    "\n",
    "def _trvaep_normalize(adata: AnnData, n_top_genes: int) -> AnnData:\n",
    "    sc.pp.normalize_per_cell(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes)\n",
    "    adata = adata[:, adata.var['highly_variable']]\n",
    "    return adata\n",
    "\n",
    "\n",
    "def run_trvaep(adata: AnnData, config: TrVAEpConfig) -> AnnData:\n",
    "    \"\"\"trVAE (PyTorch version) wrapper function. It's a slightly modified scIB code.\"\"\"\n",
    "    import trvaep\n",
    "    from scipy.sparse import issparse\n",
    "\n",
    "    n_batches = adata.obs[config.batch_key].nunique()\n",
    "\n",
    "    adata = _trvaep_normalize(adata, n_top_genes=config.n_top_genes)\n",
    "\n",
    "    # Densify the data matrix\n",
    "    if issparse(adata.X):\n",
    "        adata.X = adata.X.A\n",
    "\n",
    "    model = trvaep.CVAE(\n",
    "        adata.n_vars,\n",
    "        num_classes=n_batches,\n",
    "        encoder_layer_sizes=[config.layer1, config.layer2],  # Originally [64, 32]\n",
    "        decoder_layer_sizes=[config.layer2, config.layer1],  # Originally [32, 64]\n",
    "        latent_dim=config.n_latent,\n",
    "        alpha=config.alpha,\n",
    "        use_mmd=True,\n",
    "        beta=1,\n",
    "        output_activation=\"ReLU\",\n",
    "    )\n",
    "\n",
    "    # Note: set seed for reproducibility of results\n",
    "    trainer = trvaep.Trainer(\n",
    "        model,\n",
    "        adata,\n",
    "        condition_key=config.batch_key,\n",
    "        seed=config.seed,\n",
    "        learning_rate=config.learning_rate\n",
    "    )\n",
    "\n",
    "    trainer.train_trvae(\n",
    "        n_epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        early_patience=config.early_patience\n",
    "    )\n",
    "\n",
    "    # Get the dominant batch covariate\n",
    "    main_batch = adata.obs[config.batch_key].value_counts().idxmax()\n",
    "\n",
    "    # Get latent representation\n",
    "    latent_y = model.get_y(\n",
    "        adata.X,\n",
    "        c=model.label_encoder.transform(np.tile(np.array([main_batch]), len(adata))),\n",
    "    )\n",
    "    adata.obsm[config.latent_key] = latent_y\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "class ScGENConfig(ModelConfig):\n",
    "    name: str = \"scgen\"\n",
    "    malignant_only: bool = False  # Probably -- hard to be 100% sure\n",
    "\n",
    "\n",
    "def run_scgen(adata: AnnData, config: ScGENConfig) -> AnnData:\n",
    "    raise NotImplementedError(\"scGEN model in scIB doesn't add low-dimensional representations, \"\n",
    "                              \"so that the implementation is tricky. Moreover, it requires other cell types.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BBKNNConfig(ModelConfig):\n",
    "    name: str = \"bbknn\"\n",
    "    neighbors_within_batch: int = 3\n",
    "\n",
    "\n",
    "def run_bbknn(adata: AnnData, config: BBKNNConfig) -> AnnData:\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes, subset=True)\n",
    "    sc.pp.scale(adata)\n",
    "    sc.tl.pca(adata)\n",
    "    bbknn.bbknn(\n",
    "        adata,\n",
    "        batch_key=config.batch_key,\n",
    "        neighbors_within_batch=config.neighbors_within_batch,\n",
    "    )\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SCVIConfig(ModelConfig):\n",
    "    name: str = \"scvi\"\n",
    "    gpu: bool = True\n",
    "    covariates: Optional[List] = field(\n",
    "        default_factory=lambda: [\"S_score\", \"G2M_score\"]\n",
    "    )\n",
    "    n_latent: int = 4\n",
    "    n_hidden: int = 128\n",
    "    n_layers: int = 1\n",
    "    max_epochs: int = 400\n",
    "\n",
    "\n",
    "def run_scvi(adata: AnnData, config: SCVIConfig) -> AnnData:\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes)\n",
    "    bdata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "\n",
    "    scvi.model.SCVI.setup_anndata(bdata, layer=\"counts\", batch_key=config.batch_key,\n",
    "                                  continuous_covariate_keys=config.covariates)\n",
    "    model = scvi.model.SCVI(\n",
    "        bdata,\n",
    "        n_latent=config.n_latent,\n",
    "        n_hidden=config.n_hidden,\n",
    "        n_layers=config.n_layers,\n",
    "    )\n",
    "    model.train(\n",
    "        max_epochs=config.max_epochs,\n",
    "        # TODO: add this to cansig!\n",
    "        train_size=1.0,\n",
    "        plan_kwargs={\"n_epochs_kl_warmup\": config.max_epochs},\n",
    "    )\n",
    "    adata.obsm[config.latent_key] = model.get_latent_representation()\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScanoramaConfig(ModelConfig):\n",
    "    name: str = \"scanorama\"\n",
    "    knn: int = 20\n",
    "    sigma: float = 15.0\n",
    "    approx: bool = True\n",
    "    alpha: float = 0.1\n",
    "\n",
    "\n",
    "def run_scanorama(adata: AnnData, config: ScanoramaConfig) -> AnnData:\n",
    "    # scanorama requires that cells from the same batch must\n",
    "    # be contiguously stored in adata\n",
    "    idx = np.argsort(adata.obs[config.batch_key])\n",
    "    adata = adata[idx, :].copy()\n",
    "    sc.pp.recipe_zheng17(adata, n_top_genes=config.n_top_genes)\n",
    "    sc.tl.pca(adata)\n",
    "    sce.pp.scanorama_integrate(\n",
    "        adata,\n",
    "        config.batch_key,\n",
    "        adjusted_basis=config.latent_key,\n",
    "        knn=config.knn,\n",
    "        sigma=config.sigma,\n",
    "        approx=config.approx,\n",
    "        alpha=config.alpha,\n",
    "    )\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HarmonyConfig(ModelConfig):\n",
    "    name: str = \"harmony\"\n",
    "    max_iter_harmony: int = 100\n",
    "    max_iter_kmeans: int = 100\n",
    "    theta: float = 2.0\n",
    "    lamb: float = 1.0\n",
    "    epsilon_cluster: float = 1e-5\n",
    "    epsilon_harmony: float = 1e-4\n",
    "    random_state: int = 0\n",
    "\n",
    "\n",
    "def run_harmony(adata: AnnData, config: HarmonyConfig) -> AnnData:\n",
    "    sc.pp.recipe_zheng17(adata, n_top_genes=config.n_top_genes)\n",
    "    sc.tl.pca(adata)\n",
    "    sce.pp.harmony_integrate(\n",
    "        adata,\n",
    "        config.batch_key,\n",
    "        theta=config.theta,\n",
    "        lamb=config.lamb,\n",
    "        adjusted_basis=config.latent_key,\n",
    "        max_iter_harmony=config.max_iter_harmony,\n",
    "        max_iter_kmeans=config.max_iter_kmeans,\n",
    "        epsilon_cluster=config.epsilon_cluster,\n",
    "        epsilon_harmony=config.epsilon_harmony,\n",
    "        random_state=config.random_state,\n",
    "    )\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CanSigConfig(ModelConfig):\n",
    "    name: str = \"cansig\"\n",
    "    gpu: bool = True\n",
    "    malignant_only: bool = False\n",
    "    n_latent: int = 4\n",
    "    n_layers: int = 1\n",
    "    n_hidden: int = 128\n",
    "    n_latent_batch_effect: int = 5\n",
    "    n_latent_cnv: int = 10\n",
    "    max_epochs: int = 400\n",
    "    cnv_max_epochs: int = 400\n",
    "    batch_effect_max_epochs: int = 400\n",
    "    beta: float = 1.0\n",
    "    batch_effect_beta: float = 1.0\n",
    "    covariates: Optional[List] = field(\n",
    "        default_factory=lambda: [\"S_score\", \"G2M_score\"]\n",
    "    )\n",
    "    annealing: str = \"linear\"\n",
    "    malignant_key: str = \"malignant_key\"\n",
    "    malignant_cat: str = \"malignant\"\n",
    "    non_malignant_cat: str = \"non-malignant\"\n",
    "    subclonal_key: str = \"subclonal\"\n",
    "    celltype_key: str = \"program\"\n",
    "\n",
    "\n",
    "def run_cansig(adata: AnnData, config: CanSigConfig) -> AnnData:\n",
    "    bdata = CanSig.preprocessing(\n",
    "        adata.copy(),\n",
    "        n_highly_variable_genes=config.n_top_genes,\n",
    "        malignant_key=config.malignant_key,\n",
    "        malignant_cat=config.malignant_cat,\n",
    "    )\n",
    "    CanSig.setup_anndata(\n",
    "        bdata,\n",
    "        celltype_key=config.celltype_key,\n",
    "        malignant_key=config.malignant_key,\n",
    "        malignant_cat=config.malignant_cat,\n",
    "        non_malignant_cat=config.non_malignant_cat,\n",
    "        continuous_covariate_keys=config.covariates,\n",
    "        layer=\"counts\",\n",
    "    )\n",
    "    model = CanSig(\n",
    "        bdata,\n",
    "        n_latent=config.n_latent,\n",
    "        n_layers=config.n_layers,\n",
    "        n_hidden=config.n_hidden,\n",
    "        n_latent_cnv=config.n_latent_cnv,\n",
    "        n_latent_batch_effect=config.n_latent_batch_effect,\n",
    "        sample_id_key=config.batch_key,\n",
    "        subclonal_key=config.subclonal_key,\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        max_epochs=config.max_epochs,\n",
    "        cnv_max_epochs=config.cnv_max_epochs,\n",
    "        batch_effect_max_epochs=config.batch_effect_max_epochs,\n",
    "        train_size=1.0,\n",
    "        plan_kwargs={\n",
    "            \"n_epochs_kl_warmup\": config.max_epochs,\n",
    "            \"beta\": config.beta,\n",
    "            \"annealing\": config.annealing,\n",
    "        },\n",
    "        batch_effect_plan_kwargs={\"beta\": config.batch_effect_beta},\n",
    "    )\n",
    "\n",
    "    save_model_history(model)\n",
    "\n",
    "    save_latent_spaces(model, adata)\n",
    "\n",
    "    idx = model.get_index(malignant_cells=True)\n",
    "    adata = adata[idx, :].copy()\n",
    "    adata.obsm[config.latent_key] = model.get_latent_representation()\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MNNConfig(ModelConfig):\n",
    "    name: str = \"nmm\"\n",
    "    k: int = 20\n",
    "    sigma: float = 1.\n",
    "\n",
    "\n",
    "def run_mnn(adata: AnnData, config: MNNConfig) -> AnnData:\n",
    "    split = split_batches(adata, config.batch_key)\n",
    "\n",
    "    bdata = adata.copy()\n",
    "    sc.pp.normalize_total(bdata, target_sum=1e4)\n",
    "    sc.pp.log1p(bdata)\n",
    "    sc.pp.highly_variable_genes(bdata, n_top_genes=config.n_top_genes)\n",
    "    hvg = bdata.var.index[bdata.var[\"highly_variable\"]].tolist()\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        corrected, _, _ = sce.pp.mnn_correct(*split, var_subset=hvg)\n",
    "    corrected = corrected[0].concatenate(corrected[1:])\n",
    "\n",
    "    corrected.obsm[config.latent_key] = corrected.X\n",
    "\n",
    "    return corrected\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CombatConfig(ModelConfig):\n",
    "    name: str = \"combat\"\n",
    "    cell_cycle: bool = False\n",
    "    log_counts: bool = False\n",
    "\n",
    "\n",
    "def run_combat(adata: AnnData, config: CombatConfig) -> AnnData:\n",
    "    covariates = []\n",
    "    if config.cell_cycle:\n",
    "        covariates += [\"G2M_score\", \"S_score\"]\n",
    "\n",
    "    if config.log_counts:\n",
    "        covariates += [\"log_counts\"]\n",
    "\n",
    "    covariates = covariates or None\n",
    "\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes)\n",
    "    adata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "\n",
    "    X = sc.pp.combat(adata, config.batch_key, covariates=covariates,\n",
    "                     inplace=False)\n",
    "    adata.obsm[config.latent_key] = X\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DescConfig(ModelConfig):\n",
    "    name: str = \"desc\"\n",
    "    gpu: bool = False  # TODO: add GPU acceleration\n",
    "    res: float = 0.8\n",
    "    n_top_genes: int = 2000\n",
    "    n_neighbors: int = 10\n",
    "    batch_size: int = 256\n",
    "    tol: float = 0.005\n",
    "    learning_rate: float = 500\n",
    "    save_dir: Union[str, Path] = \".\"\n",
    "\n",
    "\n",
    "def run_desc(adata: AnnData, config: DescConfig) -> AnnData:\n",
    "    import desc\n",
    "    # Preprocessing and parameters taken from https://github.com/eleozzr/desc/issues/28.\n",
    "    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes, inplace=True)\n",
    "    sc.pp.scale(adata, zero_center=True, max_value=6)\n",
    "    adata = desc.scale_bygroup(adata, groupby=config.batch_key, max_value=6)\n",
    "    adata_out = desc.train(adata,\n",
    "                           dims=[adata.shape[1], 128, 32],  # or set 256\n",
    "                           tol=config.tol,\n",
    "                           # suggest 0.005 when the dataset less than 5000\n",
    "                           n_neighbors=config.n_neighbors,\n",
    "                           batch_size=config.batch_size,\n",
    "                           louvain_resolution=config.res,\n",
    "                           save_dir=config.save_dir,\n",
    "                           do_tsne=False,\n",
    "                           use_GPU=config.gpu,\n",
    "                           num_Cores=8,\n",
    "                           save_encoder_weights=False,\n",
    "                           save_encoder_step=2,\n",
    "                           use_ae_weights=False,\n",
    "                           do_umap=False,\n",
    "                           num_Cores_tsne=4,\n",
    "                           learning_rate=config.learning_rate)\n",
    "\n",
    "    adata_out.obsm[config.latent_key] = adata_out.obsm[\"X_Embeded_z\" + str(config.res)]\n",
    "\n",
    "    return adata_out\n",
    "\n",
    "\n",
    "def save_model_history(model: CanSig, name: str = \"\"):\n",
    "    modules = {\n",
    "        \"combined\": model.module,\n",
    "        \"batch_effect\": model.module_batch_effect,\n",
    "        \"cnv\": model.module_cnv,\n",
    "    }\n",
    "\n",
    "    for key, module in modules.items():\n",
    "        df = pd.concat([df for df in module.history.values()], axis=1)\n",
    "        df.to_csv(f\"{key}_{name}.csv\")\n",
    "\n",
    "\n",
    "def save_latent_spaces(model: CanSig, adata: AnnData, name: str = \"\"):\n",
    "    latent = model.get_batch_effect_latent_representation()\n",
    "    idx = model.get_index(malignant_cells=False)\n",
    "    df = pd.DataFrame(latent, index=adata.obs_names[idx])\n",
    "    df.to_csv(f\"{name}_batch_effect_latent.csv\")\n",
    "\n",
    "    latent = model.get_cnv_latent_representation()\n",
    "    idx = model.get_index(malignant_cells=True)\n",
    "    df = pd.DataFrame(latent, index=adata.obs_names[idx])\n",
    "    df.to_csv(f\"{name}_cnv_latent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "from anndata import AnnData\n",
    "from scETM.eval_utils import (\n",
    "    calculate_kbet,\n",
    "    _get_knn_indices,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    ")\n",
    "\n",
    "#from _cluster import LeidenNClusterConfig, LeidenNCluster\n",
    "#from models import ModelConfig\n",
    "#from utils import diffusion_nn, diffusion_conn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MetricsConfig:\n",
    "    n_neighbors: int = 50\n",
    "    group_key: str = \"program\"\n",
    "    cluster_key: str = \"leiden\"\n",
    "    n_random_seeds: int = 10\n",
    "    clustering_range: Tuple[int] = tuple(range(2, 6))\n",
    "\n",
    "\n",
    "def run_metrics(adata: AnnData, config: ModelConfig, metric_config: MetricsConfig):\n",
    "    metrics = {}\n",
    "\n",
    "    compute_neighbors(\n",
    "        adata,\n",
    "        latent_key=config.latent_key,\n",
    "        n_neighbors=metric_config.n_neighbors,\n",
    "    )\n",
    "\n",
    "    # Biological conservation metrics\n",
    "    metrics.update(compute_asw(adata, metric_config.group_key, config.latent_key))\n",
    "    metrics.update(\n",
    "        compute_davies_bouldin(adata, metric_config.group_key, config.latent_key)\n",
    "    )\n",
    "    metrics.update(\n",
    "        compute_calinski_harabasz(adata, metric_config.group_key, config.latent_key)\n",
    "    )\n",
    "    metrics.update(compute_ari_nmi(adata, metric_config))\n",
    "\n",
    "    # Batch effect metrics\n",
    "    metrics.update(\n",
    "        kbet(\n",
    "            adata,\n",
    "            latent_key=config.latent_key,\n",
    "            label_key=metric_config.group_key,\n",
    "            batch_key=config.batch_key,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def kbet(\n",
    "        adata: AnnData, latent_key: str, label_key: str, batch_key: str\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"This implementation of kBet is taken from scib and combined with the\n",
    "    kbet_single implementation from scETM.\"\"\"\n",
    "    adata.strings_to_categoricals()\n",
    "    if latent_key in adata.obsm_keys():\n",
    "        adata_tmp = sc.pp.neighbors(adata, n_neighbors=50, use_rep=latent_key,\n",
    "                                    copy=True)\n",
    "    else:\n",
    "        adata_tmp = adata.copy()\n",
    "    # check if pre-computed neighbours are stored in input file\n",
    "    connectivities = diffusion_conn(adata_tmp, min_k=50, copy=False)\n",
    "    adata_tmp.obsp[\"connectivities\"] = connectivities\n",
    "\n",
    "    # set upper bound for k0\n",
    "    size_max = 2 ** 31 - 1\n",
    "\n",
    "    # prepare call of kBET per cluster\n",
    "    kBET_scores = {\"cluster\": [], \"kBET\": []}\n",
    "    for clus in adata_tmp.obs[label_key].unique():\n",
    "\n",
    "        # subset by label\n",
    "        adata_sub = adata_tmp[adata_tmp.obs[label_key] == clus, :].copy()\n",
    "\n",
    "        # check if neighborhood size too small or only one batch in subset\n",
    "        if np.logical_or(\n",
    "                adata_sub.n_obs < 10, len(adata_sub.obs[batch_key].cat.categories) == 1\n",
    "        ):\n",
    "            print(f\"{clus} consists of a single batch or is too small. Skip.\")\n",
    "            score = np.nan\n",
    "        else:\n",
    "            quarter_mean = np.floor(\n",
    "                np.mean(adata_sub.obs[batch_key].value_counts()) / 4\n",
    "            ).astype(\"int\")\n",
    "            k0 = np.min([70, np.max([10, quarter_mean])])\n",
    "            # check k0 for reasonability\n",
    "            if k0 * adata_sub.n_obs >= size_max:\n",
    "                k0 = np.floor(size_max / adata_sub.n_obs).astype(\"int\")\n",
    "\n",
    "            n_comp, labs = scipy.sparse.csgraph.connected_components(\n",
    "                adata_sub.obsp[\"connectivities\"], connection=\"strong\"\n",
    "            )\n",
    "\n",
    "            if n_comp == 1:  # a single component to compute kBET on\n",
    "                adata_sub.obsm[\"knn_indices\"] = diffusion_nn(adata_sub, k=k0)\n",
    "                adata_sub.uns[\"neighbors\"][\"params\"][\"n_neighbors\"] = k0\n",
    "\n",
    "                score = calculate_kbet(\n",
    "                    adata_sub,\n",
    "                    use_rep=\"\",\n",
    "                    batch_col=batch_key,\n",
    "                    calc_knn=False,\n",
    "                    n_neighbors=adata_sub.uns[\"neighbors\"][\"params\"][\"n_neighbors\"],\n",
    "                )[2]\n",
    "\n",
    "            else:\n",
    "                # check the number of components where kBET can be computed upon\n",
    "                comp_size = pd.value_counts(labs)\n",
    "                # check which components are small\n",
    "                comp_size_thresh = 3 * k0\n",
    "                idx_nonan = np.flatnonzero(\n",
    "                    np.in1d(labs, comp_size[comp_size >= comp_size_thresh].index)\n",
    "                )\n",
    "\n",
    "                # check if 75% of all cells can be used for kBET run\n",
    "                if len(idx_nonan) / len(labs) >= 0.75:\n",
    "                    # create another subset of components, assume they are not visited\n",
    "                    # in a diffusion process\n",
    "                    adata_sub_sub = adata_sub[idx_nonan, :].copy()\n",
    "                    adata_sub_sub.obsm[\"knn_indices\"] = diffusion_nn(\n",
    "                        adata_sub_sub, k=k0\n",
    "                    )\n",
    "                    adata_sub_sub.uns[\"neighbors\"][\"params\"][\"n_neighbors\"] = k0\n",
    "\n",
    "                    score = calculate_kbet(\n",
    "                        adata_sub_sub,\n",
    "                        use_rep=\"\",\n",
    "                        batch_col=batch_key,\n",
    "                        calc_knn=False,\n",
    "                        n_neighbors=adata_sub_sub.uns[\"neighbors\"][\"params\"][\n",
    "                            \"n_neighbors\"\n",
    "                        ],\n",
    "                    )[2]\n",
    "\n",
    "                else:  # if there are too many too small connected components,\n",
    "                    score = 0  # i.e. 100% rejection\n",
    "\n",
    "        kBET_scores[\"cluster\"].append(clus)\n",
    "        kBET_scores[\"kBET\"].append(score)\n",
    "\n",
    "    kBET_scores = pd.DataFrame.from_dict(kBET_scores)\n",
    "    kBET_scores = kBET_scores.reset_index(drop=True)\n",
    "\n",
    "    final_score = np.nanmean(kBET_scores[\"kBET\"]).item()\n",
    "\n",
    "    return {\"k_bet_acceptance_rate\": final_score}\n",
    "\n",
    "\n",
    "def compute_ari(adata: AnnData, group_key: str, cluster_key: str) -> float:\n",
    "    return adjusted_rand_score(adata.obs[group_key], adata.obs[cluster_key])\n",
    "\n",
    "\n",
    "def compute_nmi(adata: AnnData, group_key: str, cluster_key: str) -> float:\n",
    "    return normalized_mutual_info_score(adata.obs[group_key], adata.obs[cluster_key])\n",
    "\n",
    "def compute_asw(\n",
    "        adata: AnnData, group_key: str, latent_key: str\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    if latent_key not in adata.obsm_keys():\n",
    "        return {\"average_silhouette_width\": np.nan}\n",
    "    asw = silhouette_score(X=adata.obsm[latent_key], labels=adata.obs[group_key])\n",
    "    asw = (asw + 1) / 2\n",
    "\n",
    "    return {\"average_silhouette_width\": asw}\n",
    "\n",
    "\n",
    "def compute_calinski_harabasz(\n",
    "        adata: AnnData, group_key: str, latent_key: str\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    if latent_key not in adata.obsm_keys():\n",
    "        return {\"calinski_harabasz_score\": np.nan}\n",
    "    score = calinski_harabasz_score(adata.obsm[latent_key], adata.obs[group_key])\n",
    "    return {\"calinski_harabasz_score\": score}\n",
    "\n",
    "\n",
    "def compute_davies_bouldin(\n",
    "        adata: AnnData, group_key: str, latent_key: str\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    if latent_key not in adata.obsm_keys():\n",
    "        return {\"davies_bouldin\": np.nan}\n",
    "    score = davies_bouldin_score(adata.obsm[latent_key], adata.obs[group_key])\n",
    "    return {\"davies_bouldin\": score}\n",
    "\n",
    "\n",
    "def compute_ari_nmi(\n",
    "        adata: AnnData, metric_config: MetricsConfig\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    metrics = {}\n",
    "    for k in metric_config.clustering_range:\n",
    "        for random_seed in range(metric_config.n_random_seeds):\n",
    "            try:\n",
    "                leiden_config = LeidenNClusterConfig(\n",
    "                    random_state=random_seed, clusters=k\n",
    "                )\n",
    "                cluster_algo = LeidenNCluster(leiden_config)\n",
    "                cluster_algo.fit_predict(adata, key_added=metric_config.cluster_key)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                ari = np.nan\n",
    "                nmi = np.nan\n",
    "            else:\n",
    "                ari = compute_ari(adata, metric_config.group_key,\n",
    "                                  metric_config.cluster_key)\n",
    "                nmi = compute_nmi(adata, metric_config.group_key,\n",
    "                                  metric_config.cluster_key)\n",
    "\n",
    "            metrics[f\"ari_{k}_{random_seed}\"] = ari\n",
    "            metrics[f\"nmi_{k}_{random_seed}\"] = nmi\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_neighbors(adata: AnnData, latent_key: str, n_neighbors: int):\n",
    "    if latent_key in adata.obsm.keys():\n",
    "        knn_indices = _get_knn_indices(\n",
    "            adata,\n",
    "            use_rep=latent_key,\n",
    "            n_neighbors=n_neighbors,\n",
    "            calc_knn=True,\n",
    "        )\n",
    "        adata.obsm[\"knn_indices\"] = knn_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal  # pytype: disable=not-supported-yet\n",
    "from typing import Optional\n",
    "\n",
    "import anndata as an  # pytype: disable=import-error\n",
    "import numpy as np\n",
    "import pydantic  # pytype: disable=import-error\n",
    "import scanpy as sc  # pytype: disable=import-error\n",
    "from anndata import AnnData\n",
    "\n",
    "_SupportedMetric = Literal[\n",
    "    \"cityblock\",\n",
    "    \"cosine\",\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"manhattan\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "    \"chebyshev\",\n",
    "    \"correlation\",\n",
    "    \"dice\",\n",
    "    \"hamming\",\n",
    "    \"jaccard\",\n",
    "    \"kulsinski\",\n",
    "    \"mahalanobis\",\n",
    "    \"minkowski\",\n",
    "    \"rogerstanimoto\",\n",
    "    \"russellrao\",\n",
    "    \"seuclidean\",\n",
    "    \"sokalmichener\",\n",
    "    \"sokalsneath\",\n",
    "    \"sqeuclidean\",\n",
    "    \"yule\",\n",
    "]\n",
    "\n",
    "\n",
    "class NeighborsGraphConfig(pydantic.BaseModel):\n",
    "    \"\"\"Settings for neighborhood graph computation.\n",
    "    For description, see\n",
    "    https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html\n",
    "    \"\"\"\n",
    "\n",
    "    n_neighbors: int = pydantic.Field(default=15)\n",
    "    n_pcs: Optional[int] = pydantic.Field(default=None)\n",
    "    knn: bool = pydantic.Field(default=True)\n",
    "    # TODO(Pawel): Check whether we can support other methods as well.\n",
    "    method: Literal[\"umap\"] = pydantic.Field(default=\"umap\")\n",
    "    metric: _SupportedMetric = pydantic.Field(default=\"euclidean\")\n",
    "\n",
    "\n",
    "class _LeidenBaseConfig(pydantic.BaseModel):\n",
    "    nngraph: NeighborsGraphConfig = pydantic.Field(default_factory=NeighborsGraphConfig)\n",
    "    random_state: int = pydantic.Field(default=0)\n",
    "    directed: bool = pydantic.Field(default=True)\n",
    "    use_weights: bool = pydantic.Field(default=True)\n",
    "    n_iterations: int = pydantic.Field(default=-1)\n",
    "\n",
    "\n",
    "class BinSearchSettings(pydantic.BaseModel):\n",
    "    start: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3, description=\"The minimal resolution.\"\n",
    "    )\n",
    "    end: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=10.0, description=\"The maximal resolution.\"\n",
    "    )\n",
    "    epsilon: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3,\n",
    "        description=\"Controls the maximal number of iterations before throwing lookup \"\n",
    "        \"error.\",\n",
    "    )\n",
    "\n",
    "    @pydantic.validator(\"end\")\n",
    "    def validate_end_greater_than_start(cls, v, values, **kwargs) -> float:\n",
    "        if v <= values[\"start\"]:\n",
    "            raise ValueError(\"In binary search end must be greater than start.\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class LeidenNClusterConfig(_LeidenBaseConfig):\n",
    "    clusters: int = pydantic.Field(\n",
    "        default=5, description=\"The number of clusters to be returned.\"\n",
    "    )\n",
    "    binsearch: BinSearchSettings = pydantic.Field(default_factory=BinSearchSettings)\n",
    "\n",
    "\n",
    "class LeidenNCluster:\n",
    "    def __init__(self, settings: LeidenNClusterConfig) -> None:\n",
    "        self._settings = settings\n",
    "\n",
    "    def fit_predict(self, adata: AnnData, key_added: str) -> np.ndarray:\n",
    "        for offset in [0, 20_000, 30_000, 40_000]:\n",
    "            points = _binary_search_leiden_resolution(\n",
    "                adata,\n",
    "                k=self._settings.clusters,\n",
    "                key_added=key_added,\n",
    "                random_state=self._settings.random_state + offset,\n",
    "                directed=self._settings.directed,\n",
    "                use_weights=self._settings.use_weights,\n",
    "                start=self._settings.binsearch.start,\n",
    "                end=self._settings.binsearch.end,\n",
    "                _epsilon=self._settings.binsearch.epsilon,\n",
    "            )\n",
    "            if points is not None:\n",
    "                break\n",
    "        # In case that for multiple random seeds we didn't find a resolution that\n",
    "        # matches the number of clusters, we raise a ValueError.\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No resolution for the number of clusters {self._settings.clusters}\"\n",
    "                f\" found.\"\n",
    "            )\n",
    "\n",
    "        return points.obs[key_added].astype(int).values\n",
    "\n",
    "\n",
    "def _binary_search_leiden_resolution(\n",
    "    adata: an.AnnData,\n",
    "    k: int,\n",
    "    start: float,\n",
    "    end: float,\n",
    "    key_added: str,\n",
    "    random_state: int,\n",
    "    directed: bool,\n",
    "    use_weights: bool,\n",
    "    _epsilon: float,\n",
    ") -> Optional[an.AnnData]:\n",
    "    \"\"\"Binary search to get the resolution corresponding\n",
    "    to the right k.\"\"\"\n",
    "    # We try the resolution which is in the middle of the interval\n",
    "    res = 0.5 * (start + end)\n",
    "\n",
    "    # Run Leiden clustering\n",
    "    sc.tl.leiden(\n",
    "        adata,\n",
    "        resolution=res,\n",
    "        key_added=key_added,\n",
    "        random_state=random_state,\n",
    "        directed=directed,\n",
    "        use_weights=use_weights,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Get the number of clusters found\n",
    "    selected_k = adata.obs[key_added].nunique()\n",
    "    if selected_k == k:\n",
    "        return adata\n",
    "\n",
    "    # If the start and the end are too close (and there is no point in doing another\n",
    "    # iteration), we raise an error that one can't find the required number of clusters\n",
    "    if abs(end - start) < _epsilon * res:\n",
    "        return None\n",
    "\n",
    "    if selected_k > k:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=start,\n",
    "            end=res,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )\n",
    "    else:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=res,\n",
    "            end=end,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_samples, silhouette_score\n",
    "\n",
    "def silhouette_batch(\n",
    "    adata,\n",
    "    batch_key,\n",
    "    group_key,\n",
    "    latent_key,\n",
    "    metric=\"euclidean\",\n",
    "    return_all=False,\n",
    "    scale=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Batch ASW\n",
    "    Modified average silhouette width (ASW) of batch\n",
    "    This metric measures the silhouette of a given batch.\n",
    "    It assumes that a silhouette width close to 0 represents perfect overlap of the batches, thus the absolute value of\n",
    "    the silhouette width is used to measure how well batches are mixed.\n",
    "    For all cells :math:`i` of a cell type :math:`C_j`, the batch ASW of that cell type is:\n",
    "    .. math::\n",
    "        batch \\\\, ASW_j = \\\\frac{1}{|C_j|} \\\\sum_{i \\\\in C_j} |silhouette(i)|\n",
    "    The final score is the average of the absolute silhouette widths computed per cell type :math:`M`.\n",
    "    .. math::\n",
    "        batch \\\\, ASW = \\\\frac{1}{|M|} \\\\sum_{i \\\\in M} batch \\\\, ASW_j\n",
    "    For a scaled metric (which is the default), the absolute ASW per group is subtracted from 1 before averaging, so that\n",
    "    0 indicates suboptimal label representation and 1 indicates optimal label representation.\n",
    "    .. math::\n",
    "        batch \\\\, ASW_j = \\\\frac{1}{|C_j|} \\\\sum_{i \\\\in C_j} 1 - |silhouette(i)|\n",
    "    :param batch_key: batch labels to be compared against\n",
    "    :param group_key: group labels to be subset by e.g. cell type\n",
    "    :param embed: name of column in adata.obsm\n",
    "    :param metric: see sklearn silhouette score\n",
    "    :param scale: if True, scale between 0 and 1\n",
    "    :param return_all: if True, return all silhouette scores and label means\n",
    "        default False: return average width silhouette (ASW)\n",
    "    :param verbose: print silhouette score per group\n",
    "    :return:\n",
    "        Batch ASW  (always)\n",
    "        Mean silhouette per group in pd.DataFrame (additionally, if return_all=True)\n",
    "        Absolute silhouette scores per group label (additionally, if return_all=True)\n",
    "    \"\"\"\n",
    "    if latent_key not in adata.obsm.keys():\n",
    "        print(adata.obsm.keys())\n",
    "        raise KeyError(f\"{latent_key} not in obsm\")\n",
    "\n",
    "    sil_per_label = []\n",
    "    for group in adata.obs[group_key].unique():\n",
    "        adata_group = adata[adata.obs[group_key] == group]\n",
    "        n_batches = adata_group.obs[batch_key].nunique()\n",
    "\n",
    "        if (n_batches == 1) or (n_batches == adata_group.shape[0]):\n",
    "            continue\n",
    "\n",
    "        sil = silhouette_samples(\n",
    "            adata_group.obsm[latent_key], adata_group.obs[batch_key], metric=metric\n",
    "        )\n",
    "\n",
    "        # take only absolute value\n",
    "        sil = [abs(i) for i in sil]\n",
    "\n",
    "        if scale:\n",
    "            # scale s.t. highest number is optimal\n",
    "            sil = [1 - i for i in sil]\n",
    "\n",
    "        sil_per_label.extend([(group, score) for score in sil])\n",
    "\n",
    "    sil_df = pd.DataFrame.from_records(\n",
    "        sil_per_label, columns=[\"group\", \"silhouette_score\"]\n",
    "    )\n",
    "\n",
    "    if len(sil_per_label) == 0:\n",
    "        sil_means = np.nan\n",
    "        asw = np.nan\n",
    "    else:\n",
    "        sil_means = sil_df.groupby(\"group\").mean()\n",
    "        asw = sil_means[\"silhouette_score\"].mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"mean silhouette per group: {sil_means}\")\n",
    "\n",
    "    if return_all:\n",
    "        return asw, sil_means, sil_df\n",
    "\n",
    "    return {\"asw_batch_score\":asw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbet_rni_asw(adata, latent_key, batch_key, label_key, group_key, max_clusters):\n",
    "    bdata = adata.copy()\n",
    "    ari_score_collection = []\n",
    "    k = np.linspace(2, max_clusters, max_clusters-1)\n",
    "    for i in k:\n",
    "        cdata = _binary_search_leiden_resolution(bdata, k = int(i), start = 0.1, end = 0.9, key_added ='final_annotation', random_state = 0, directed = False, \n",
    "        use_weights = False, _epsilon = 1e-3)\n",
    "        if cdata is None:\n",
    "            ari_score_collection.append(0)\n",
    "            continue\n",
    "        adata.obs['cluster_{}'.format(int(i))] = cdata.obs['final_annotation']\n",
    "        ari_score_collection.append(compute_ari(adata, group_key = group_key, cluster_key = 'cluster_{}'.format(int(i))))\n",
    "\n",
    "\n",
    "    # Note, all keys should come from the columns in adata.obs\n",
    "    ari_score = {f\"maximum ARI_score with {int(k[np.argmax(ari_score_collection)])} clusters\": np.max(ari_score_collection)}\n",
    "    sc.pl.umap(adata, color = ['cluster_{}'.format(int(k[np.argmax(ari_score_collection)]))])\n",
    "    kbet_score = kbet(adata, latent_key=latent_key, batch_key=batch_key, label_key=label_key)\n",
    "    asw_score = compute_asw(adata, group_key = group_key, latent_key = latent_key)\n",
    "    asw_batch_score = silhouette_batch(adata, batch_key = batch_key, group_key= group_key, latent_key= latent_key)\n",
    "\n",
    "    return [kbet_score, ari_score, asw_score, asw_batch_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If returned an error called \"Equality comparisons are not supported for AnnData objects, instead compare the desired attributes.\", this means laiden clustering \n",
    "# returned bdata as a Nonetype object, just need to decrease the max_clusters\n",
    "#kbet_rni_asw(adata = mdata, latent_key = 'X_VAE', batch_key = 'batch', label_key = 'final_annotation', group_key = 'final_annotation', max_clusters = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_scale(dataset):\n",
    "    if np.max(dataset) - np.min(dataset) != 0:\n",
    "        return (dataset - np.min(dataset)) / (np.max(dataset) - np.min(dataset))\n",
    "    if np.max(dataset) - np.min(dataset) == 0:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_list_generator(layers, num_latent, lr):\n",
    "    grid_search_list = []\n",
    "    for i in range(len(layers)):\n",
    "        for j in range(len(num_latent)):\n",
    "            for k in range(len(lr)):\n",
    "                grid_search_list.append([layers[i], num_latent[j], lr[k]])\n",
    "\n",
    "    return grid_search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_list = grid_search_list_generator(layers = [1,3,5], \n",
    "num_latent = [5,10,15], \n",
    "lr =[1e-3,1e-4,1e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = []\n",
    "for i in grid_search_list:\n",
    "    col_name.append(\"VAE layers of %s, num latents of %s, lr of %s\" % (i[0],i[1],i[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_general_scvi(adata, layer_key, batch_key, label_key, group_key, max_clusters, grid_search_list):\n",
    "    # After malignant_cell_collection and model_preprocessing\n",
    "\n",
    "\n",
    "    ari_collection = []\n",
    "    asw_batch_collection = []\n",
    "    kbet_collection = [] \n",
    "    asw_collection = []\n",
    "\n",
    "    for i in grid_search_list:\n",
    "        model = model_train(adata, layer_key = layer_key, batch_key = batch_key, n_layers = i[0], n_hidden = 512, n_latent = i[1], lr = i[2])\n",
    "        print(\"VAE layers of %s, num latents of %s, lr of %s\" % (i[0],i[1],i[2]))\n",
    "        latent_key = get_latent_UMAP(model, adata, batch_key, label_key, added_latent_key = 'X_VAE_{}'.format(i), print_UMAP = True)\n",
    "        score_collection = kbet_rni_asw(adata, latent_key = latent_key, batch_key = batch_key, label_key = label_key, group_key = group_key, max_clusters = max_clusters)\n",
    "        for i in score_collection[1].values():\n",
    "            ari_collection.append(i)\n",
    "        for i in score_collection[3].values():\n",
    "            asw_batch_collection.append(i)\n",
    "        for i in score_collection[0].values():\n",
    "            kbet_collection.append(i)\n",
    "        for i in score_collection[2].values():\n",
    "            asw_collection.append(i)\n",
    "\n",
    "    ari_collection_mn = max_min_scale(ari_collection)\n",
    "    asw_batch_collection_mn = max_min_scale(asw_batch_collection)\n",
    "    kbet_collection_mn = max_min_scale(kbet_collection)\n",
    "    asw_collection_mn = max_min_scale(asw_collection)\n",
    "\n",
    "\n",
    "    #batch removal score includes kbet and asw_batch, bio-conservation score (cell_type_keeping) includes ari and asw_cell\n",
    "    bio_score_collection = [] \n",
    "    batch_score_collection = [] \n",
    "    overall_score_collection = []\n",
    "    for i in range(len(grid_search_list)):\n",
    "        bio_score = np.mean((ari_collection_mn[i], asw_collection_mn[i]))\n",
    "        bio_score_collection.append(bio_score)\n",
    "        batch_score = np.mean((kbet_collection_mn[i], asw_batch_collection_mn[i]))\n",
    "        batch_score_collection.append(batch_score)\n",
    "        overall_score_collection.append(0.6 * bio_score + 0.4 * batch_score)\n",
    "    return [ari_collection, asw_collection, kbet_collection, asw_batch_collection,bio_score_collection, batch_score_collection, overall_score_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = sc.read_h5ad(\"Immune_ALL_human.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = malignant_cell_collection(idata, malignant_cell_incices = [1,5], label_key = 'final_annotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preprocessing(mdata, 'batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = hyperparameter_tuning_general_scvi(mdata, layer_key = 'counts', batch_key= 'batch', label_key= 'final_annotation', group_key= 'final_annotation', max_clusters = 8, grid_search_list = grid_search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_scorelist_into_df(scorelist, variable_name, store, csv_file_name):\n",
    "    score_pd = pd.DataFrame(scorelist, index = [\"ari\", \"asw_cell\", \"kbet\", \"asw_batch\",\"bio_score\", \"batch_score\", \"overall_score\"], columns = variable_name)\n",
    "    if store:\n",
    "        score_pd.to_csv(csv_file_name)\n",
    "    return score_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pd = convert_scorelist_into_df(score, col_name, True,'grid_general_scvi_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata = sc.read_h5ad(\"Lung_atlas_public.h5ad\")\n",
    "mldata = malignant_cell_collection(ldata, malignant_cell_incices = [0,2], label_key = 'cell_type')\n",
    "model_preprocessing(mldata, 'batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lung = hyperparameter_tuning_general_scvi(mldata, layer_key = 'counts', batch_key= 'batch', label_key= 'cell_type', group_key= 'cell_type', max_clusters = 8, grid_search_list = grid_search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lung_pd = convert_scorelist_into_df(score_lung, col_name, True,'grid_general_scvi_lung.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = sc.read_h5ad(\"human_pancreas_norm_complexBatch.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdata = malignant_cell_collection(pdata, malignant_cell_incices = [0,2], label_key = 'celltype')\n",
    "model_preprocessing(mpdata, 'tech')\n",
    "score_pancreas = hyperparameter_tuning_general_scvi(mpdata, layer_key = 'counts', batch_key= 'tech', label_key= 'celltype', group_key= 'celltype', max_clusters = 8, grid_search_list = grid_search_list)\n",
    "score_pancreas_pd = convert_scorelist_into_df(score_pancreas, col_name, True,'grid_general_scvi_pancreas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_scvi2",
   "language": "python",
   "name": "lab_scvi2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1fcf5ce4f39e09cd072220dda92b1b07d2539b28e330b861d32713f27d750be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
