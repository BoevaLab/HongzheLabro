{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scvi\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from math import ceil\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scvi.dataloaders import DataSplitter\n",
    "from scvi.train import TrainingPlan, AdversarialTrainingPlan, TrainRunner\n",
    "\n",
    "\n",
    "def _check_warmup(\n",
    "    plan_kwargs: Dict[str, Any],\n",
    "    max_epochs: int,\n",
    "    n_cells: int,\n",
    "    batch_size: int,\n",
    "    train_size: float = 1.0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Raises a warning if the max_kl_weight is not reached by the end of training.\n",
    "    Parameters\n",
    "    ----------\n",
    "    plan_kwargs\n",
    "        Keyword args for :class:`~scvi.train.TrainingPlan`.\n",
    "    max_epochs\n",
    "        Number of passes through the dataset.\n",
    "    n_cells\n",
    "        Number of cells in the whole datasets.\n",
    "    batch_size\n",
    "        Minibatch size to use during training.\n",
    "    train_size\n",
    "        Fraction of cells used for training.\n",
    "    \"\"\"\n",
    "    _WARNING_MESSAGE = (\n",
    "        \"max_{mode}={max} is less than n_{mode}_kl_warmup={warm_up}. \"\n",
    "        \"The max_kl_weight will not be reached during training.\"\n",
    "    )\n",
    "\n",
    "    n_steps_kl_warmup = plan_kwargs.get(\"n_steps_kl_warmup\", None)\n",
    "    n_epochs_kl_warmup = plan_kwargs.get(\"n_epochs_kl_warmup\", None)\n",
    "\n",
    "    # The only time n_steps_kl_warmup is used is when n_epochs_kl_warmup is explicitly\n",
    "    # set to None. This also catches the case when both n_epochs_kl_warmup and\n",
    "    # n_steps_kl_warmup are set to None and max_kl_weight will always be reached.\n",
    "    if (\n",
    "        \"n_epochs_kl_warmup\" in plan_kwargs\n",
    "        and plan_kwargs[\"n_epochs_kl_warmup\"] is None\n",
    "    ):\n",
    "        n_cell_train = ceil(train_size * n_cells)\n",
    "        steps_per_epoch = n_cell_train // batch_size + (n_cell_train % batch_size >= 3)\n",
    "        max_steps = max_epochs * steps_per_epoch\n",
    "        if n_steps_kl_warmup and max_steps < n_steps_kl_warmup:\n",
    "            warnings.warn(\n",
    "                _WARNING_MESSAGE.format(\n",
    "                    mode=\"steps\", max=max_steps, warm_up=n_steps_kl_warmup\n",
    "                )\n",
    "            )\n",
    "    elif n_epochs_kl_warmup:\n",
    "        if max_epochs < n_epochs_kl_warmup:\n",
    "            warnings.warn(\n",
    "                _WARNING_MESSAGE.format(\n",
    "                    mode=\"epochs\", max=max_epochs, warm_up=n_epochs_kl_warmup\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        if max_epochs < 400:\n",
    "            warnings.warn(\n",
    "                _WARNING_MESSAGE.format(mode=\"epochs\", max=max_epochs, warm_up=400)\n",
    "            )\n",
    "\n",
    "class UnsupervisedadverTrainingMixin_imm:\n",
    "    def train(\n",
    "        self,\n",
    "        max_epochs: Optional[int] = None,\n",
    "        use_gpu: Optional[Union[str, int, bool]] = None,\n",
    "        train_size: float = 0.9,\n",
    "        validation_size: Optional[float] = None,\n",
    "        batch_size: int = 128,\n",
    "        early_stopping: bool = False,\n",
    "        plan_kwargs: Optional[dict] = None,\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_epochs\n",
    "            Number of passes through the dataset. If `None`, defaults to\n",
    "            `np.min([round((20000 / n_cells) * 400), 400])`\n",
    "        use_gpu\n",
    "            Use default GPU if available (if None or True), or index of GPU to use (if int),\n",
    "            or name of GPU (if str, e.g., `'cuda:0'`), or use CPU (if False).\n",
    "        train_size\n",
    "            Size of training set in the range [0.0, 1.0].\n",
    "        validation_size\n",
    "            Size of the test set. If `None`, defaults to 1 - `train_size`. If\n",
    "            `train_size + validation_size < 1`, the remaining cells belong to a test set.\n",
    "        batch_size\n",
    "            Minibatch size to use during training.\n",
    "        early_stopping\n",
    "            Perform early stopping. Additional arguments can be passed in `**kwargs`.\n",
    "            See :class:`~scvi.train.Trainer` for further options.\n",
    "        plan_kwargs\n",
    "            Keyword args for :class:`~scvi.train.TrainingPlan`. Keyword arguments passed to\n",
    "            `train()` will overwrite values present in `plan_kwargs`, when appropriate.\n",
    "        **trainer_kwargs\n",
    "            Other keyword args for :class:`~scvi.train.Trainer`.\n",
    "        \"\"\"\n",
    "        n_cells = self.adata.n_obs\n",
    "        if max_epochs is None:\n",
    "            max_epochs = int(np.min([round((20000 / n_cells) * 400), 400]))\n",
    "\n",
    "        plan_kwargs = plan_kwargs if isinstance(plan_kwargs, dict) else dict()\n",
    "\n",
    "        _check_warmup(plan_kwargs, max_epochs, n_cells, batch_size)\n",
    "\n",
    "        data_splitter = DataSplitter(\n",
    "            self.adata_manager,\n",
    "            train_size=train_size,\n",
    "            validation_size=validation_size,\n",
    "            batch_size=batch_size,\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "        training_plan = TrainingPlan(self.module, lr=0.01, lr_patience = 30, **plan_kwargs)\n",
    "\n",
    "        es = \"early_stopping\"\n",
    "        trainer_kwargs[es] = (\n",
    "            early_stopping if es not in trainer_kwargs.keys() else trainer_kwargs[es]\n",
    "        )\n",
    "        runner = TrainRunner(\n",
    "            self,\n",
    "            training_plan=training_plan,\n",
    "            data_splitter=data_splitter,\n",
    "            max_epochs=max_epochs,\n",
    "            use_gpu=use_gpu,\n",
    "            **trainer_kwargs,\n",
    "        )\n",
    "        return runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from anndata import AnnData\n",
    "from scvi.module import VAE\n",
    "from scvi.model.base import VAEMixin, BaseModelClass, UnsupervisedTrainingMixin\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.data import AnnDataManager\n",
    "from scvi.data.fields import (\n",
    "    LayerField,\n",
    "    CategoricalObsField,\n",
    "    NumericalObsField,\n",
    "    CategoricalJointObsField,\n",
    "    NumericalJointObsField,\n",
    ")\n",
    "from scvi.nn import DecoderSCVI\n",
    "\n",
    "class SCVIadver_humanimm(VAEMixin, UnsupervisedadverTrainingMixin_imm, BaseModelClass):\n",
    "    \"\"\"\n",
    "    single-cell Variational Inference [Lopez18]_.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_latent: int = 10,\n",
    "        #n_hidden: int = 256,\n",
    "        #n_layers: int = 5,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "\n",
    "        self.module = VAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            #n_hidden=n_hidden,\n",
    "            #n_layers=n_layers,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "\n",
    "        self._model_summary_string = (\n",
    "            \"SCVI Model with the following params: \\nn_latent: {}\"\n",
    "        ).format(\n",
    "            n_latent,\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    @classmethod\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        batch_key: Optional[str] = None,\n",
    "        labels_key: Optional[str] = None,\n",
    "        layer: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> Optional[AnnData]:\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            # Dummy fields required for VAE class.\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, labels_key),\n",
    "            NumericalObsField(\n",
    "                REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False\n",
    "            ),\n",
    "            CategoricalJointObsField(\n",
    "                REGISTRY_KEYS.CAT_COVS_KEY, None\n",
    "            ),\n",
    "            NumericalJointObsField(\n",
    "                REGISTRY_KEYS.CONT_COVS_KEY, None\n",
    "            ),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(\n",
    "            fields=anndata_fields, setup_method_args=setup_method_args\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spliting_del_round(adata, label_key, non_malignant_cell_indices, malignant_cell_indices, delete):\n",
    "    ######################################################## choosing to eliminate the cell_types\n",
    "    \n",
    "    # Pretending some of the cell_types are maglingant\n",
    "    if \"counts\" not in adata.layers.keys():\n",
    "        adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "    # check whether the cell_type has float expression data, if so, delete it\n",
    "    if delete:\n",
    "        for ind,val in enumerate(non_malignant_cell_indices):\n",
    "            checkdata = adata[adata.obs[label_key].isin(adata.obs[label_key].unique()[[val]])]\n",
    "\n",
    "            if scipy.sparse.issparse(checkdata.layers['counts']):\n",
    "                if np.any([(k%1) for k in checkdata.layers['counts'].todense().ravel()]):\n",
    "                    non_malignant_cell_indices[ind] = -1\n",
    "            else:\n",
    "                if np.any([(k%1) for k in checkdata.layers['counts'].ravel()]):\n",
    "                    non_malignant_cell_indices[ind] = -1\n",
    "    \n",
    "        non_malignant_cell_indices = [i for i in non_malignant_cell_indices if i != -1]\n",
    "\n",
    "        for ind,val in enumerate(malignant_cell_indices):\n",
    "            checkdata = adata[adata.obs[label_key].isin(adata.obs[label_key].unique()[[val]])]\n",
    "\n",
    "            if scipy.sparse.issparse(checkdata.layers['counts']):\n",
    "                if np.any([(k%1) for k in checkdata.layers['counts'].todense().ravel()]):\n",
    "                    malignant_cell_indices[ind] = -1\n",
    "            else:\n",
    "                if np.any([(k%1) for k in checkdata.layers['counts'].ravel()]):\n",
    "                    malignant_cell_indices[ind] = -1\n",
    "\n",
    "        malignant_cell_indices = [i for i in malignant_cell_indices if i != -1]\n",
    "\n",
    "    # check whether the cell_type has float expression data, if so, round it        \n",
    "    else:\n",
    "        if scipy.sparse.issparse(adata.layers['counts']):\n",
    "            if np.any([(k%1) for k in adata.layers['counts'].todense().ravel()]):\n",
    "                adata.layers['counts'] = np.round(adata.layers['counts'].todense())\n",
    "        else:\n",
    "            if np.any([(k%1) for k in adata.layers['counts'].ravel()]):\n",
    "                adata.layers['counts'] = np.round(adata.layers['counts'])\n",
    "\n",
    "\n",
    "\n",
    "    ndata = adata[adata.obs[label_key].isin(adata.obs[label_key].unique()[[i for i in non_malignant_cell_indices]])]\n",
    "    mdata = adata[adata.obs[label_key].isin(adata.obs[label_key].unique()[[i for i in malignant_cell_indices]])]\n",
    "    return ndata, mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_firstVAE(ndata, layer_key, cell_type_key):\n",
    "    ndata = ndata.copy()\n",
    "    SCVIadver_humanimm.setup_anndata(ndata, layer = layer_key, batch_key = cell_type_key) \n",
    "    model_imm = SCVIadver_humanimm(ndata)\n",
    "    model_imm.train(max_epochs=100)\n",
    "    return model_imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_UMAP(model, ndata, batch_key, label_key, added_latent_key, print_UMAP):\n",
    "    latent = model.get_latent_representation()\n",
    "    ndata.obsm[added_latent_key] = latent\n",
    "    sc.pp.neighbors(ndata, use_rep=added_latent_key, n_neighbors=20)\n",
    "    sc.tl.umap(ndata, min_dist=0.3)\n",
    "    if print_UMAP:\n",
    "        sc.pl.umap(ndata, color = [label_key, batch_key])\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch_information(ndata, mdata, batch_key, latent_info):\n",
    "    batch_df_imm = pd.DataFrame(latent_info, index = ndata.obs[batch_key])\n",
    "    batch_df_mean_imm = batch_df_imm.groupby(batch_df_imm.index).mean()\n",
    "    batch_df_mean_loc_imm = batch_df_mean_imm.loc[mdata.obs[batch_key]]\n",
    "    latent_id_imm = [f'latent{i}' for i in range(batch_df_mean_loc_imm.shape[1])]\n",
    "    mdata.obs[latent_id_imm] = batch_df_mean_loc_imm.values\n",
    "    return mdata, latent_id_imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_VAE(adata, batch_key, label_key, layer_key, non_malignant_cell_indices, malignant_cell_indices, delete, added_latent_key, print_UMAP):\n",
    "    ndata, mdata = data_spliting_del_round(adata, label_key = label_key, non_malignant_cell_indices = non_malignant_cell_indices, malignant_cell_indices = malignant_cell_indices, delete = delete)\n",
    "    model_imm = train_model_firstVAE(ndata, layer_key = layer_key, cell_type_key = label_key)\n",
    "    latent_imm = get_latent_UMAP(model_imm, ndata, batch_key = batch_key, label_key = label_key, added_latent_key = added_latent_key, print_UMAP = print_UMAP)\n",
    "    mdata, latent_id_imm = fetch_batch_information(ndata, mdata, batch_key = batch_key, latent_info = latent_imm)\n",
    "    return mdata, latent_id_imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_VAE(mdata, latent_info, n_layers, n_hidden, n_latent, lr):\n",
    "    scvi.model.SCVI.setup_anndata(mdata, layer='counts', continuous_covariate_keys=latent_info)\n",
    "    sec_model_imm = scvi.model.SCVI(mdata, n_layers = n_layers, n_hidden = n_hidden, n_latent = n_latent)\n",
    "    sec_model_imm.train(max_epochs = 100, validation_size = 0.1, check_val_every_n_epoch = 5, early_stopping=True, \n",
    "    early_stopping_monitor='elbo_validation', early_stopping_patience = 20, plan_kwargs={'lr':lr})\n",
    "    return sec_model_imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction_loss_and_elbo(model):\n",
    "    train_recon_loss = model.history['reconstruction_loss_train']\n",
    "    elbo_train = model.history['elbo_train']\n",
    "    elbo_val = model.history['elbo_validation']\n",
    "    val_recon_loss = model.history['reconstruction_loss_validation']\n",
    "    ax = train_recon_loss.plot()\n",
    "    elbo_train.plot(ax = ax)\n",
    "    elbo_val.plot(ax = ax)\n",
    "    val_recon_loss.plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_secUMAP(model, ndata, batch_key, label_key, added_latent_key, print_UMAP):\n",
    "    latent = model.get_latent_representation()\n",
    "    ndata.obsm[added_latent_key] = latent\n",
    "    sc.pp.neighbors(ndata, use_rep=added_latent_key, n_neighbors=20)\n",
    "    sc.tl.umap(ndata, min_dist=0.3)\n",
    "    if print_UMAP:\n",
    "        sc.pl.umap(ndata, color = [label_key, batch_key])\n",
    "    return added_latent_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def save_latent(adata: AnnData, latent_key: str, dataset_name: str) -> None:\n",
    "    if latent_key in adata.obsm_keys():\n",
    "        latent = pd.DataFrame(adata.obsm[latent_key], index=adata.obs_names)\n",
    "        latent.to_csv(f\"{dataset_name}_latent.csv\")\n",
    "\n",
    "def plot_integration(\n",
    "        adata: AnnData, dataset_name: str, batch_key: str, group_key: str\n",
    ") -> None:\n",
    "    sc.settings.figdir = \".\"\n",
    "    sc.tl.umap(adata)\n",
    "    sc.pl.umap(\n",
    "        adata, color=[batch_key, group_key], save=f\"_{dataset_name}.png\", show=False\n",
    "    )\n",
    "\n",
    "\n",
    "def get_partition(gpu: bool) -> str:\n",
    "    if gpu:\n",
    "        return \"gpu\"\n",
    "    return \"compute\"\n",
    "\n",
    "\n",
    "def get_gres(gpu: bool) -> Optional[str]:\n",
    "    if gpu:\n",
    "        return \"gpu:rtx2080ti:1\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def diffusion_nn(adata, k, max_iterations=26):\n",
    "    \"\"\"\n",
    "    Diffusion neighbourhood score\n",
    "    This function generates a nearest neighbour list from a connectivities matrix\n",
    "    as supplied by BBKNN or Conos. This allows us to select a consistent number\n",
    "    of nearest neighbours across all methods.\n",
    "    Return:\n",
    "       `k_indices` a numpy.ndarray of the indices of the k-nearest neighbors.\n",
    "    \"\"\"\n",
    "    if \"neighbors\" not in adata.uns:\n",
    "        raise ValueError(\n",
    "            \"`neighbors` not in adata object. \" \"Please compute a neighbourhood graph!\"\n",
    "        )\n",
    "\n",
    "    if \"connectivities\" not in adata.obsp:\n",
    "        raise ValueError(\n",
    "            \"`connectivities` not in `adata.obsp`. \"\n",
    "            \"Please pass an object with connectivities computed!\"\n",
    "        )\n",
    "\n",
    "    T = adata.obsp[\"connectivities\"]\n",
    "\n",
    "    # Row-normalize T\n",
    "    T = sparse.diags(1 / T.sum(1).A.ravel()) * T\n",
    "\n",
    "    T_agg = T ** 3\n",
    "    M = T + T ** 2 + T_agg\n",
    "    i = 4\n",
    "\n",
    "    while ((M > 0).sum(1).min() < (k + 1)) and (i < max_iterations):\n",
    "        # note: k+1 is used as diag is non-zero (self-loops)\n",
    "        print(f\"Adding diffusion to step {i}\")\n",
    "        T_agg *= T\n",
    "        M += T_agg\n",
    "        i += 1\n",
    "\n",
    "    if (M > 0).sum(1).min() < (k + 1):\n",
    "        raise ValueError(\n",
    "            f\"could not find {k} nearest neighbors in {max_iterations}\"\n",
    "            \"diffusion steps.\\n Please increase max_iterations or reduce\"\n",
    "            \" k.\\n\"\n",
    "        )\n",
    "\n",
    "    M.setdiag(0)\n",
    "    k_indices = np.argpartition(M.A, -k, axis=1)[:, -k:]\n",
    "\n",
    "    return k_indices\n",
    "\n",
    "\n",
    "def diffusion_conn(adata, min_k=50, copy=True, max_iterations=26):\n",
    "    \"\"\"\n",
    "    Diffusion for connectivites matrix extension\n",
    "    This function performs graph diffusion on the connectivities matrix until a\n",
    "    minimum number `min_k` of entries per row are non-zero.\n",
    "    Note:\n",
    "    Due to self-loops min_k-1 non-zero connectivies entries is actually the stopping\n",
    "    criterion. This is equivalent to `sc.pp.neighbors`.\n",
    "    Returns:\n",
    "       The diffusion-enhanced connectivities matrix of a copy of the AnnData object\n",
    "       with the diffusion-enhanced connectivities matrix is in\n",
    "       `adata.uns[\"neighbors\"][\"conectivities\"]`\n",
    "    \"\"\"\n",
    "    if \"neighbors\" not in adata.uns:\n",
    "        raise ValueError(\n",
    "            \"`neighbors` not in adata object. \" \"Please compute a neighbourhood graph!\"\n",
    "        )\n",
    "\n",
    "    if \"connectivities\" not in adata.obsp:\n",
    "        raise ValueError(\n",
    "            \"`connectivities` not in `adata.obsp`. \"\n",
    "            \"Please pass an object with connectivities computed!\"\n",
    "        )\n",
    "\n",
    "    T = adata.obsp[\"connectivities\"]\n",
    "\n",
    "    # Normalize T with max row sum\n",
    "    # Note: This keeps the matrix symmetric and ensures |M| doesn't keep growing\n",
    "    T = sparse.diags(1 / np.array([T.sum(1).max()] * T.shape[0])) * T\n",
    "\n",
    "    M = T\n",
    "\n",
    "    # Check for disconnected component\n",
    "    n_comp, labs = sparse.csgraph.connected_components(\n",
    "        adata.obsp[\"connectivities\"], connection=\"strong\"\n",
    "    )\n",
    "\n",
    "    if n_comp > 1:\n",
    "        tab = pd.value_counts(labs)\n",
    "        small_comps = tab.index[tab < min_k]\n",
    "        large_comp_mask = np.array(~pd.Series(labs).isin(small_comps))\n",
    "    else:\n",
    "        large_comp_mask = np.array([True] * M.shape[0])\n",
    "\n",
    "    T_agg = T\n",
    "    i = 2\n",
    "    while ((M[large_comp_mask, :][:, large_comp_mask] > 0).sum(1).min() < min_k) and (\n",
    "            i < max_iterations\n",
    "    ):\n",
    "        print(f\"Adding diffusion to step {i}\")\n",
    "        T_agg *= T\n",
    "        M += T_agg\n",
    "        i += 1\n",
    "\n",
    "    if (M[large_comp_mask, :][:, large_comp_mask] > 0).sum(1).min() < min_k:\n",
    "        raise ValueError(\n",
    "            \"could not create diffusion connectivities matrix\"\n",
    "            f\"with at least {min_k} non-zero entries in\"\n",
    "            f\"{max_iterations} iterations.\\n Please increase the\"\n",
    "            \"value of max_iterations or reduce k_min.\\n\"\n",
    "        )\n",
    "\n",
    "    M.setdiag(0)\n",
    "\n",
    "    if copy:\n",
    "        adata_tmp = adata.copy()\n",
    "        adata_tmp.uns[\"neighbors\"].update({\"diffusion_connectivities\": M})\n",
    "        return adata_tmp\n",
    "\n",
    "    else:\n",
    "        return M\n",
    "\n",
    "\n",
    "def split_batches(adata: AnnData, batch_key: str) -> List[AnnData]:\n",
    "    splits = []\n",
    "    for batch in adata.obs[batch_key].cat.categories:\n",
    "        splits.append(adata[adata.obs[batch_key] == batch].copy())\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal  # pytype: disable=not-supported-yet\n",
    "from typing import Optional\n",
    "\n",
    "import anndata as an  # pytype: disable=import-error\n",
    "import numpy as np\n",
    "import pydantic  # pytype: disable=import-error\n",
    "import scanpy as sc  # pytype: disable=import-error\n",
    "from anndata import AnnData\n",
    "\n",
    "_SupportedMetric = Literal[\n",
    "    \"cityblock\",\n",
    "    \"cosine\",\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"manhattan\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "    \"chebyshev\",\n",
    "    \"correlation\",\n",
    "    \"dice\",\n",
    "    \"hamming\",\n",
    "    \"jaccard\",\n",
    "    \"kulsinski\",\n",
    "    \"mahalanobis\",\n",
    "    \"minkowski\",\n",
    "    \"rogerstanimoto\",\n",
    "    \"russellrao\",\n",
    "    \"seuclidean\",\n",
    "    \"sokalmichener\",\n",
    "    \"sokalsneath\",\n",
    "    \"sqeuclidean\",\n",
    "    \"yule\",\n",
    "]\n",
    "\n",
    "\n",
    "class NeighborsGraphConfig(pydantic.BaseModel):\n",
    "    \"\"\"Settings for neighborhood graph computation.\n",
    "    For description, see\n",
    "    https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html\n",
    "    \"\"\"\n",
    "\n",
    "    n_neighbors: int = pydantic.Field(default=15)\n",
    "    n_pcs: Optional[int] = pydantic.Field(default=None)\n",
    "    knn: bool = pydantic.Field(default=True)\n",
    "    # TODO(Pawel): Check whether we can support other methods as well.\n",
    "    method: Literal[\"umap\"] = pydantic.Field(default=\"umap\")\n",
    "    metric: _SupportedMetric = pydantic.Field(default=\"euclidean\")\n",
    "\n",
    "\n",
    "class _LeidenBaseConfig(pydantic.BaseModel):\n",
    "    nngraph: NeighborsGraphConfig = pydantic.Field(default_factory=NeighborsGraphConfig)\n",
    "    random_state: int = pydantic.Field(default=0)\n",
    "    directed: bool = pydantic.Field(default=True)\n",
    "    use_weights: bool = pydantic.Field(default=True)\n",
    "    n_iterations: int = pydantic.Field(default=-1)\n",
    "\n",
    "\n",
    "class BinSearchSettings(pydantic.BaseModel):\n",
    "    start: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3, description=\"The minimal resolution.\"\n",
    "    )\n",
    "    end: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=10.0, description=\"The maximal resolution.\"\n",
    "    )\n",
    "    epsilon: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3,\n",
    "        description=\"Controls the maximal number of iterations before throwing lookup \"\n",
    "        \"error.\",\n",
    "    )\n",
    "\n",
    "    @pydantic.validator(\"end\")\n",
    "    def validate_end_greater_than_start(cls, v, values, **kwargs) -> float:\n",
    "        if v <= values[\"start\"]:\n",
    "            raise ValueError(\"In binary search end must be greater than start.\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class LeidenNClusterConfig(_LeidenBaseConfig):\n",
    "    clusters: int = pydantic.Field(\n",
    "        default=5, description=\"The number of clusters to be returned.\"\n",
    "    )\n",
    "    binsearch: BinSearchSettings = pydantic.Field(default_factory=BinSearchSettings)\n",
    "\n",
    "\n",
    "class LeidenNCluster:\n",
    "    def __init__(self, settings: LeidenNClusterConfig) -> None:\n",
    "        self._settings = settings\n",
    "\n",
    "    def fit_predict(self, adata: AnnData, key_added: str) -> np.ndarray:\n",
    "        for offset in [0, 20_000, 30_000, 40_000]:\n",
    "            points = _binary_search_leiden_resolution(\n",
    "                adata,\n",
    "                k=self._settings.clusters,\n",
    "                key_added=key_added,\n",
    "                random_state=self._settings.random_state + offset,\n",
    "                directed=self._settings.directed,\n",
    "                use_weights=self._settings.use_weights,\n",
    "                start=self._settings.binsearch.start,\n",
    "                end=self._settings.binsearch.end,\n",
    "                _epsilon=self._settings.binsearch.epsilon,\n",
    "            )\n",
    "            if points is not None:\n",
    "                break\n",
    "        # In case that for multiple random seeds we didn't find a resolution that\n",
    "        # matches the number of clusters, we raise a ValueError.\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No resolution for the number of clusters {self._settings.clusters}\"\n",
    "                f\" found.\"\n",
    "            )\n",
    "\n",
    "        return points.obs[key_added].astype(int).values\n",
    "\n",
    "\n",
    "def _binary_search_leiden_resolution(\n",
    "    adata: an.AnnData,\n",
    "    k: int,\n",
    "    start: float,\n",
    "    end: float,\n",
    "    key_added: str,\n",
    "    random_state: int,\n",
    "    directed: bool,\n",
    "    use_weights: bool,\n",
    "    _epsilon: float,\n",
    ") -> Optional[an.AnnData]:\n",
    "    \"\"\"Binary search to get the resolution corresponding\n",
    "    to the right k.\"\"\"\n",
    "    # We try the resolution which is in the middle of the interval\n",
    "    res = 0.5 * (start + end)\n",
    "\n",
    "    # Run Leiden clustering\n",
    "    sc.tl.leiden(\n",
    "        adata,\n",
    "        resolution=res,\n",
    "        key_added=key_added,\n",
    "        random_state=random_state,\n",
    "        directed=directed,\n",
    "        use_weights=use_weights,\n",
    "    )\n",
    "\n",
    "    # Get the number of clusters found\n",
    "    selected_k = adata.obs[key_added].nunique()\n",
    "    if selected_k == k:\n",
    "        return adata\n",
    "\n",
    "    # If the start and the end are too close (and there is no point in doing another\n",
    "    # iteration), we raise an error that one can't find the required number of clusters\n",
    "    if abs(end - start) < _epsilon * res:\n",
    "        return None\n",
    "\n",
    "    if selected_k > k:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=start,\n",
    "            end=res,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )\n",
    "    else:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=res,\n",
    "            end=end,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List, Optional, Union\n",
    "\n",
    "import bbknn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scanpy.external as sce\n",
    "import scvi\n",
    "from anndata import AnnData\n",
    "from cansig.integration.model import CanSig\n",
    "from omegaconf import MISSING\n",
    "\n",
    "#from utils import split_batches\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str = MISSING\n",
    "    gpu: bool = False\n",
    "    malignant_only: bool = True\n",
    "    batch_key: str = \"sample_id\"\n",
    "    latent_key: str = \"latent\"\n",
    "    n_top_genes: int = 2000\n",
    "\n",
    "\n",
    "\n",
    "def run_model(adata: AnnData, cfg) -> Tuple[AnnData, float]:\n",
    "    start = timer()\n",
    "    if cfg.name == \"bbknn\":\n",
    "        adata = run_bbknn(adata, config=cfg)\n",
    "    elif cfg.name == \"scvi\":\n",
    "        adata = run_scvi(adata, config=cfg)\n",
    "    elif cfg.name == \"scanorama\":\n",
    "        adata = run_scanorama(adata, config=cfg)\n",
    "    elif cfg.name == \"harmony\":\n",
    "        adata = run_harmony(adata, config=cfg)\n",
    "    elif cfg.name == \"cansig\":\n",
    "        adata = run_cansig(adata, config=cfg)\n",
    "    elif cfg.name == \"nmm\":\n",
    "        adata = run_mnn(adata, config=cfg)\n",
    "    elif cfg.name == \"combat\":\n",
    "        adata = run_combat(adata, config=cfg)\n",
    "    elif cfg.name == \"desc\":\n",
    "        adata = run_desc(adata, config=cfg)\n",
    "    elif cfg.name == \"dhaka\":\n",
    "        adata = run_dhaka(adata, config=cfg)\n",
    "    elif cfg.name == \"scanvi\":\n",
    "        adata = run_scanvi(adata, config=cfg)\n",
    "    elif cfg.name == \"trvaep\":\n",
    "        adata = run_trvaep(adata, config=cfg)\n",
    "    elif cfg.name == \"scgen\":\n",
    "        adata = run_scgen(adata, config=cfg)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{cfg.name} is not implemented.\")\n",
    "    run_time = timer() - start\n",
    "    return adata, run_time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DhakaConfig(ModelConfig):\n",
    "    name: str = \"dhaka\"\n",
    "    gpu: bool = True\n",
    "\n",
    "    n_latent: int = 3\n",
    "    # Data preprocessing\n",
    "    n_genes: int = 5000\n",
    "    total_expression: float = 1e6\n",
    "    pseudocounts: int = 1\n",
    "    # Training\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 50\n",
    "    learning_rate: float = 1e-4\n",
    "    clip_norm: float = 2.0\n",
    "    # Magic flag\n",
    "    scale_reconstruction_loss: bool = True\n",
    "\n",
    "\n",
    "def run_dhaka(adata: AnnData, config: DhakaConfig) -> AnnData:\n",
    "    import dhaka.api as dh\n",
    "\n",
    "    new_config = dh.DhakaConfig(\n",
    "        n_latent=config.n_latent,\n",
    "        n_genes=config.n_genes,\n",
    "        total_expression=config.total_expression,\n",
    "        pseudocounts=config.pseudocounts,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        learning_rate=config.learning_rate,\n",
    "        clip_norm=config.clip_norm,\n",
    "        scale_reconstruction_loss=config.scale_reconstruction_loss\n",
    "    )\n",
    "\n",
    "    return dh.run_dhaka(adata, config=new_config, key_added=config.latent_key)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScanVIConfig(ModelConfig):\n",
    "    name: str = \"scanvi\"\n",
    "    malignant_only: bool = False\n",
    "\n",
    "\n",
    "def run_scanvi(adata: AnnData, config: ScanVIConfig) -> AnnData:\n",
    "    raise NotImplementedError(\"This method requires several celltypes to run.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrVAEpConfig(ModelConfig):\n",
    "    name: str = \"trvaep\"\n",
    "    n_top_genes: int = 3000\n",
    "    n_latent: int = 10\n",
    "    alpha: float = 1e-4\n",
    "    layer1: int = 64\n",
    "    layer2: int = 32\n",
    "    seed: int = 42  # Random seed\n",
    "    # Training params\n",
    "    epochs: int = 300\n",
    "    batch_size: int = 1024\n",
    "    early_patience: int = 50\n",
    "    learning_rate: float = 1e-3\n",
    "\n",
    "\n",
    "def _trvaep_normalize(adata: AnnData, n_top_genes: int) -> AnnData:\n",
    "    sc.pp.normalize_per_cell(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes)\n",
    "    adata = adata[:, adata.var['highly_variable']]\n",
    "    return adata\n",
    "\n",
    "\n",
    "def run_trvaep(adata: AnnData, config: TrVAEpConfig) -> AnnData:\n",
    "    \"\"\"trVAE (PyTorch version) wrapper function. It's a slightly modified scIB code.\"\"\"\n",
    "    import trvaep\n",
    "    from scipy.sparse import issparse\n",
    "\n",
    "    n_batches = adata.obs[config.batch_key].nunique()\n",
    "\n",
    "    adata = _trvaep_normalize(adata, n_top_genes=config.n_top_genes)\n",
    "\n",
    "    # Densify the data matrix\n",
    "    if issparse(adata.X):\n",
    "        adata.X = adata.X.A\n",
    "\n",
    "    model = trvaep.CVAE(\n",
    "        adata.n_vars,\n",
    "        num_classes=n_batches,\n",
    "        encoder_layer_sizes=[config.layer1, config.layer2],  # Originally [64, 32]\n",
    "        decoder_layer_sizes=[config.layer2, config.layer1],  # Originally [32, 64]\n",
    "        latent_dim=config.n_latent,\n",
    "        alpha=config.alpha,\n",
    "        use_mmd=True,\n",
    "        beta=1,\n",
    "        output_activation=\"ReLU\",\n",
    "    )\n",
    "\n",
    "    # Note: set seed for reproducibility of results\n",
    "    trainer = trvaep.Trainer(\n",
    "        model,\n",
    "        adata,\n",
    "        condition_key=config.batch_key,\n",
    "        seed=config.seed,\n",
    "        learning_rate=config.learning_rate\n",
    "    )\n",
    "\n",
    "    trainer.train_trvae(\n",
    "        n_epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        early_patience=config.early_patience\n",
    "    )\n",
    "\n",
    "    # Get the dominant batch covariate\n",
    "    main_batch = adata.obs[config.batch_key].value_counts().idxmax()\n",
    "\n",
    "    # Get latent representation\n",
    "    latent_y = model.get_y(\n",
    "        adata.X,\n",
    "        c=model.label_encoder.transform(np.tile(np.array([main_batch]), len(adata))),\n",
    "    )\n",
    "    adata.obsm[config.latent_key] = latent_y\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "class ScGENConfig(ModelConfig):\n",
    "    name: str = \"scgen\"\n",
    "    malignant_only: bool = False  # Probably -- hard to be 100% sure\n",
    "\n",
    "\n",
    "def run_scgen(adata: AnnData, config: ScGENConfig) -> AnnData:\n",
    "    raise NotImplementedError(\"scGEN model in scIB doesn't add low-dimensional representations, \"\n",
    "                              \"so that the implementation is tricky. Moreover, it requires other cell types.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BBKNNConfig(ModelConfig):\n",
    "    name: str = \"bbknn\"\n",
    "    neighbors_within_batch: int = 3\n",
    "\n",
    "\n",
    "def run_bbknn(adata: AnnData, config: BBKNNConfig) -> AnnData:\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes, subset=True)\n",
    "    sc.pp.scale(adata)\n",
    "    sc.tl.pca(adata)\n",
    "    bbknn.bbknn(\n",
    "        adata,\n",
    "        batch_key=config.batch_key,\n",
    "        neighbors_within_batch=config.neighbors_within_batch,\n",
    "    )\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SCVIConfig(ModelConfig):\n",
    "    name: str = \"scvi\"\n",
    "    gpu: bool = True\n",
    "    covariates: Optional[List] = field(\n",
    "        default_factory=lambda: [\"S_score\", \"G2M_score\"]\n",
    "    )\n",
    "    n_latent: int = 4\n",
    "    n_hidden: int = 128\n",
    "    n_layers: int = 1\n",
    "    max_epochs: int = 400\n",
    "\n",
    "\n",
    "def run_scvi(adata: AnnData, config: SCVIConfig) -> AnnData:\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes)\n",
    "    bdata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "\n",
    "    scvi.model.SCVI.setup_anndata(bdata, layer=\"counts\", batch_key=config.batch_key,\n",
    "                                  continuous_covariate_keys=config.covariates)\n",
    "    model = scvi.model.SCVI(\n",
    "        bdata,\n",
    "        n_latent=config.n_latent,\n",
    "        n_hidden=config.n_hidden,\n",
    "        n_layers=config.n_layers,\n",
    "    )\n",
    "    model.train(\n",
    "        max_epochs=config.max_epochs,\n",
    "        # TODO: add this to cansig!\n",
    "        train_size=1.0,\n",
    "        plan_kwargs={\"n_epochs_kl_warmup\": config.max_epochs},\n",
    "    )\n",
    "    adata.obsm[config.latent_key] = model.get_latent_representation()\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScanoramaConfig(ModelConfig):\n",
    "    name: str = \"scanorama\"\n",
    "    knn: int = 20\n",
    "    sigma: float = 15.0\n",
    "    approx: bool = True\n",
    "    alpha: float = 0.1\n",
    "\n",
    "\n",
    "def run_scanorama(adata: AnnData, config: ScanoramaConfig) -> AnnData:\n",
    "    # scanorama requires that cells from the same batch must\n",
    "    # be contiguously stored in adata\n",
    "    idx = np.argsort(adata.obs[config.batch_key])\n",
    "    adata = adata[idx, :].copy()\n",
    "    sc.pp.recipe_zheng17(adata, n_top_genes=config.n_top_genes)\n",
    "    sc.tl.pca(adata)\n",
    "    sce.pp.scanorama_integrate(\n",
    "        adata,\n",
    "        config.batch_key,\n",
    "        adjusted_basis=config.latent_key,\n",
    "        knn=config.knn,\n",
    "        sigma=config.sigma,\n",
    "        approx=config.approx,\n",
    "        alpha=config.alpha,\n",
    "    )\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HarmonyConfig(ModelConfig):\n",
    "    name: str = \"harmony\"\n",
    "    max_iter_harmony: int = 100\n",
    "    max_iter_kmeans: int = 100\n",
    "    theta: float = 2.0\n",
    "    lamb: float = 1.0\n",
    "    epsilon_cluster: float = 1e-5\n",
    "    epsilon_harmony: float = 1e-4\n",
    "    random_state: int = 0\n",
    "\n",
    "\n",
    "def run_harmony(adata: AnnData, config: HarmonyConfig) -> AnnData:\n",
    "    sc.pp.recipe_zheng17(adata, n_top_genes=config.n_top_genes)\n",
    "    sc.tl.pca(adata)\n",
    "    sce.pp.harmony_integrate(\n",
    "        adata,\n",
    "        config.batch_key,\n",
    "        theta=config.theta,\n",
    "        lamb=config.lamb,\n",
    "        adjusted_basis=config.latent_key,\n",
    "        max_iter_harmony=config.max_iter_harmony,\n",
    "        max_iter_kmeans=config.max_iter_kmeans,\n",
    "        epsilon_cluster=config.epsilon_cluster,\n",
    "        epsilon_harmony=config.epsilon_harmony,\n",
    "        random_state=config.random_state,\n",
    "    )\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CanSigConfig(ModelConfig):\n",
    "    name: str = \"cansig\"\n",
    "    gpu: bool = True\n",
    "    malignant_only: bool = False\n",
    "    n_latent: int = 4\n",
    "    n_layers: int = 1\n",
    "    n_hidden: int = 128\n",
    "    n_latent_batch_effect: int = 5\n",
    "    n_latent_cnv: int = 10\n",
    "    max_epochs: int = 400\n",
    "    cnv_max_epochs: int = 400\n",
    "    batch_effect_max_epochs: int = 400\n",
    "    beta: float = 1.0\n",
    "    batch_effect_beta: float = 1.0\n",
    "    covariates: Optional[List] = field(\n",
    "        default_factory=lambda: [\"S_score\", \"G2M_score\"]\n",
    "    )\n",
    "    annealing: str = \"linear\"\n",
    "    malignant_key: str = \"malignant_key\"\n",
    "    malignant_cat: str = \"malignant\"\n",
    "    non_malignant_cat: str = \"non-malignant\"\n",
    "    subclonal_key: str = \"subclonal\"\n",
    "    celltype_key: str = \"program\"\n",
    "\n",
    "\n",
    "def run_cansig(adata: AnnData, config: CanSigConfig) -> AnnData:\n",
    "    bdata = CanSig.preprocessing(\n",
    "        adata.copy(),\n",
    "        n_highly_variable_genes=config.n_top_genes,\n",
    "        malignant_key=config.malignant_key,\n",
    "        malignant_cat=config.malignant_cat,\n",
    "    )\n",
    "    CanSig.setup_anndata(\n",
    "        bdata,\n",
    "        celltype_key=config.celltype_key,\n",
    "        malignant_key=config.malignant_key,\n",
    "        malignant_cat=config.malignant_cat,\n",
    "        non_malignant_cat=config.non_malignant_cat,\n",
    "        continuous_covariate_keys=config.covariates,\n",
    "        layer=\"counts\",\n",
    "    )\n",
    "    model = CanSig(\n",
    "        bdata,\n",
    "        n_latent=config.n_latent,\n",
    "        n_layers=config.n_layers,\n",
    "        n_hidden=config.n_hidden,\n",
    "        n_latent_cnv=config.n_latent_cnv,\n",
    "        n_latent_batch_effect=config.n_latent_batch_effect,\n",
    "        sample_id_key=config.batch_key,\n",
    "        subclonal_key=config.subclonal_key,\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        max_epochs=config.max_epochs,\n",
    "        cnv_max_epochs=config.cnv_max_epochs,\n",
    "        batch_effect_max_epochs=config.batch_effect_max_epochs,\n",
    "        train_size=1.0,\n",
    "        plan_kwargs={\n",
    "            \"n_epochs_kl_warmup\": config.max_epochs,\n",
    "            \"beta\": config.beta,\n",
    "            \"annealing\": config.annealing,\n",
    "        },\n",
    "        batch_effect_plan_kwargs={\"beta\": config.batch_effect_beta},\n",
    "    )\n",
    "\n",
    "    save_model_history(model)\n",
    "\n",
    "    save_latent_spaces(model, adata)\n",
    "\n",
    "    idx = model.get_index(malignant_cells=True)\n",
    "    adata = adata[idx, :].copy()\n",
    "    adata.obsm[config.latent_key] = model.get_latent_representation()\n",
    "\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MNNConfig(ModelConfig):\n",
    "    name: str = \"nmm\"\n",
    "    k: int = 20\n",
    "    sigma: float = 1.\n",
    "\n",
    "\n",
    "def run_mnn(adata: AnnData, config: MNNConfig) -> AnnData:\n",
    "    split = split_batches(adata, config.batch_key)\n",
    "\n",
    "    bdata = adata.copy()\n",
    "    sc.pp.normalize_total(bdata, target_sum=1e4)\n",
    "    sc.pp.log1p(bdata)\n",
    "    sc.pp.highly_variable_genes(bdata, n_top_genes=config.n_top_genes)\n",
    "    hvg = bdata.var.index[bdata.var[\"highly_variable\"]].tolist()\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        corrected, _, _ = sce.pp.mnn_correct(*split, var_subset=hvg)\n",
    "    corrected = corrected[0].concatenate(corrected[1:])\n",
    "\n",
    "    corrected.obsm[config.latent_key] = corrected.X\n",
    "\n",
    "    return corrected\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CombatConfig(ModelConfig):\n",
    "    name: str = \"combat\"\n",
    "    cell_cycle: bool = False\n",
    "    log_counts: bool = False\n",
    "\n",
    "\n",
    "def run_combat(adata: AnnData, config: CombatConfig) -> AnnData:\n",
    "    covariates = []\n",
    "    if config.cell_cycle:\n",
    "        covariates += [\"G2M_score\", \"S_score\"]\n",
    "\n",
    "    if config.log_counts:\n",
    "        covariates += [\"log_counts\"]\n",
    "\n",
    "    covariates = covariates or None\n",
    "\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes)\n",
    "    adata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "\n",
    "    X = sc.pp.combat(adata, config.batch_key, covariates=covariates,\n",
    "                     inplace=False)\n",
    "    adata.obsm[config.latent_key] = X\n",
    "    return adata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DescConfig(ModelConfig):\n",
    "    name: str = \"desc\"\n",
    "    gpu: bool = False  # TODO: add GPU acceleration\n",
    "    res: float = 0.8\n",
    "    n_top_genes: int = 2000\n",
    "    n_neighbors: int = 10\n",
    "    batch_size: int = 256\n",
    "    tol: float = 0.005\n",
    "    learning_rate: float = 500\n",
    "    save_dir: Union[str, Path] = \".\"\n",
    "\n",
    "\n",
    "def run_desc(adata: AnnData, config: DescConfig) -> AnnData:\n",
    "    import desc\n",
    "    # Preprocessing and parameters taken from https://github.com/eleozzr/desc/issues/28.\n",
    "    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=config.n_top_genes, inplace=True)\n",
    "    sc.pp.scale(adata, zero_center=True, max_value=6)\n",
    "    adata = desc.scale_bygroup(adata, groupby=config.batch_key, max_value=6)\n",
    "    adata_out = desc.train(adata,\n",
    "                           dims=[adata.shape[1], 128, 32],  # or set 256\n",
    "                           tol=config.tol,\n",
    "                           # suggest 0.005 when the dataset less than 5000\n",
    "                           n_neighbors=config.n_neighbors,\n",
    "                           batch_size=config.batch_size,\n",
    "                           louvain_resolution=config.res,\n",
    "                           save_dir=config.save_dir,\n",
    "                           do_tsne=False,\n",
    "                           use_GPU=config.gpu,\n",
    "                           num_Cores=8,\n",
    "                           save_encoder_weights=False,\n",
    "                           save_encoder_step=2,\n",
    "                           use_ae_weights=False,\n",
    "                           do_umap=False,\n",
    "                           num_Cores_tsne=4,\n",
    "                           learning_rate=config.learning_rate)\n",
    "\n",
    "    adata_out.obsm[config.latent_key] = adata_out.obsm[\"X_Embeded_z\" + str(config.res)]\n",
    "\n",
    "    return adata_out\n",
    "\n",
    "\n",
    "def save_model_history(model: CanSig, name: str = \"\"):\n",
    "    modules = {\n",
    "        \"combined\": model.module,\n",
    "        \"batch_effect\": model.module_batch_effect,\n",
    "        \"cnv\": model.module_cnv,\n",
    "    }\n",
    "\n",
    "    for key, module in modules.items():\n",
    "        df = pd.concat([df for df in module.history.values()], axis=1)\n",
    "        df.to_csv(f\"{key}_{name}.csv\")\n",
    "\n",
    "\n",
    "def save_latent_spaces(model: CanSig, adata: AnnData, name: str = \"\"):\n",
    "    latent = model.get_batch_effect_latent_representation()\n",
    "    idx = model.get_index(malignant_cells=False)\n",
    "    df = pd.DataFrame(latent, index=adata.obs_names[idx])\n",
    "    df.to_csv(f\"{name}_batch_effect_latent.csv\")\n",
    "\n",
    "    latent = model.get_cnv_latent_representation()\n",
    "    idx = model.get_index(malignant_cells=True)\n",
    "    df = pd.DataFrame(latent, index=adata.obs_names[idx])\n",
    "    df.to_csv(f\"{name}_cnv_latent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "from anndata import AnnData\n",
    "from scETM.eval_utils import (\n",
    "    calculate_kbet,\n",
    "    _get_knn_indices,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    ")\n",
    "\n",
    "#from _cluster import LeidenNClusterConfig, LeidenNCluster\n",
    "#from models import ModelConfig\n",
    "#from utils import diffusion_nn, diffusion_conn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MetricsConfig:\n",
    "    n_neighbors: int = 50\n",
    "    group_key: str = \"program\"\n",
    "    cluster_key: str = \"leiden\"\n",
    "    n_random_seeds: int = 10\n",
    "    clustering_range: Tuple[int] = tuple(range(2, 6))\n",
    "\n",
    "\n",
    "def run_metrics(adata: AnnData, config: ModelConfig, metric_config: MetricsConfig):\n",
    "    metrics = {}\n",
    "\n",
    "    compute_neighbors(\n",
    "        adata,\n",
    "        latent_key=config.latent_key,\n",
    "        n_neighbors=metric_config.n_neighbors,\n",
    "    )\n",
    "\n",
    "    # Biological conservation metrics\n",
    "    metrics.update(compute_asw(adata, metric_config.group_key, config.latent_key))\n",
    "    metrics.update(\n",
    "        compute_davies_bouldin(adata, metric_config.group_key, config.latent_key)\n",
    "    )\n",
    "    metrics.update(\n",
    "        compute_calinski_harabasz(adata, metric_config.group_key, config.latent_key)\n",
    "    )\n",
    "    metrics.update(compute_ari_nmi(adata, metric_config))\n",
    "\n",
    "    # Batch effect metrics\n",
    "    metrics.update(\n",
    "        kbet(\n",
    "            adata,\n",
    "            latent_key=config.latent_key,\n",
    "            label_key=metric_config.group_key,\n",
    "            batch_key=config.batch_key,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def kbet(\n",
    "        adata: AnnData, latent_key: str, label_key: str, batch_key: str\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"This implementation of kBet is taken from scib and combined with the\n",
    "    kbet_single implementation from scETM.\"\"\"\n",
    "    adata.strings_to_categoricals()\n",
    "    if latent_key in adata.obsm_keys():\n",
    "        adata_tmp = sc.pp.neighbors(adata, n_neighbors=50, use_rep=latent_key,\n",
    "                                    copy=True)\n",
    "    else:\n",
    "        adata_tmp = adata.copy()\n",
    "    # check if pre-computed neighbours are stored in input file\n",
    "    connectivities = diffusion_conn(adata_tmp, min_k=50, copy=False)\n",
    "    adata_tmp.obsp[\"connectivities\"] = connectivities\n",
    "\n",
    "    # set upper bound for k0\n",
    "    size_max = 2 ** 31 - 1\n",
    "\n",
    "    # prepare call of kBET per cluster\n",
    "    kBET_scores = {\"cluster\": [], \"kBET\": []}\n",
    "    for clus in adata_tmp.obs[label_key].unique():\n",
    "\n",
    "        # subset by label\n",
    "        adata_sub = adata_tmp[adata_tmp.obs[label_key] == clus, :].copy()\n",
    "\n",
    "        # check if neighborhood size too small or only one batch in subset\n",
    "        if np.logical_or(\n",
    "                adata_sub.n_obs < 10, len(adata_sub.obs[batch_key].cat.categories) == 1\n",
    "        ):\n",
    "            print(f\"{clus} consists of a single batch or is too small. Skip.\")\n",
    "            score = np.nan\n",
    "        else:\n",
    "            quarter_mean = np.floor(\n",
    "                np.mean(adata_sub.obs[batch_key].value_counts()) / 4\n",
    "            ).astype(\"int\")\n",
    "            k0 = np.min([70, np.max([10, quarter_mean])])\n",
    "            # check k0 for reasonability\n",
    "            if k0 * adata_sub.n_obs >= size_max:\n",
    "                k0 = np.floor(size_max / adata_sub.n_obs).astype(\"int\")\n",
    "\n",
    "            n_comp, labs = scipy.sparse.csgraph.connected_components(\n",
    "                adata_sub.obsp[\"connectivities\"], connection=\"strong\"\n",
    "            )\n",
    "\n",
    "            if n_comp == 1:  # a single component to compute kBET on\n",
    "                adata_sub.obsm[\"knn_indices\"] = diffusion_nn(adata_sub, k=k0)\n",
    "                adata_sub.uns[\"neighbors\"][\"params\"][\"n_neighbors\"] = k0\n",
    "\n",
    "                score = calculate_kbet(\n",
    "                    adata_sub,\n",
    "                    use_rep=\"\",\n",
    "                    batch_col=batch_key,\n",
    "                    calc_knn=False,\n",
    "                    n_neighbors=adata_sub.uns[\"neighbors\"][\"params\"][\"n_neighbors\"],\n",
    "                )[2]\n",
    "\n",
    "            else:\n",
    "                # check the number of components where kBET can be computed upon\n",
    "                comp_size = pd.value_counts(labs)\n",
    "                # check which components are small\n",
    "                comp_size_thresh = 3 * k0\n",
    "                idx_nonan = np.flatnonzero(\n",
    "                    np.in1d(labs, comp_size[comp_size >= comp_size_thresh].index)\n",
    "                )\n",
    "\n",
    "                # check if 75% of all cells can be used for kBET run\n",
    "                if len(idx_nonan) / len(labs) >= 0.75:\n",
    "                    # create another subset of components, assume they are not visited\n",
    "                    # in a diffusion process\n",
    "                    adata_sub_sub = adata_sub[idx_nonan, :].copy()\n",
    "                    adata_sub_sub.obsm[\"knn_indices\"] = diffusion_nn(\n",
    "                        adata_sub_sub, k=k0\n",
    "                    )\n",
    "                    adata_sub_sub.uns[\"neighbors\"][\"params\"][\"n_neighbors\"] = k0\n",
    "\n",
    "                    score = calculate_kbet(\n",
    "                        adata_sub_sub,\n",
    "                        use_rep=\"\",\n",
    "                        batch_col=batch_key,\n",
    "                        calc_knn=False,\n",
    "                        n_neighbors=adata_sub_sub.uns[\"neighbors\"][\"params\"][\n",
    "                            \"n_neighbors\"\n",
    "                        ],\n",
    "                    )[2]\n",
    "\n",
    "                else:  # if there are too many too small connected components,\n",
    "                    score = 0  # i.e. 100% rejection\n",
    "\n",
    "        kBET_scores[\"cluster\"].append(clus)\n",
    "        kBET_scores[\"kBET\"].append(score)\n",
    "\n",
    "    kBET_scores = pd.DataFrame.from_dict(kBET_scores)\n",
    "    kBET_scores = kBET_scores.reset_index(drop=True)\n",
    "\n",
    "    final_score = np.nanmean(kBET_scores[\"kBET\"]).item()\n",
    "\n",
    "    return {\"k_bet_acceptance_rate\": final_score}\n",
    "\n",
    "\n",
    "def compute_ari(adata: AnnData, group_key: str, cluster_key: str) -> float:\n",
    "    return adjusted_rand_score(adata.obs[group_key], adata.obs[cluster_key])\n",
    "\n",
    "\n",
    "def compute_nmi(adata: AnnData, group_key: str, cluster_key: str) -> float:\n",
    "    return normalized_mutual_info_score(adata.obs[group_key], adata.obs[cluster_key])\n",
    "\n",
    "def compute_asw(\n",
    "        adata: AnnData, group_key: str, latent_key: str\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    if latent_key not in adata.obsm_keys():\n",
    "        return {\"average_silhouette_width\": np.nan}\n",
    "    asw = silhouette_score(X=adata.obsm[latent_key], labels=adata.obs[group_key])\n",
    "    asw = (asw + 1) / 2\n",
    "\n",
    "    return {\"average_silhouette_width\": asw}\n",
    "\n",
    "\n",
    "def compute_calinski_harabasz(\n",
    "        adata: AnnData, group_key: str, latent_key: str\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    if latent_key not in adata.obsm_keys():\n",
    "        return {\"calinski_harabasz_score\": np.nan}\n",
    "    score = calinski_harabasz_score(adata.obsm[latent_key], adata.obs[group_key])\n",
    "    return {\"calinski_harabasz_score\": score}\n",
    "\n",
    "\n",
    "def compute_davies_bouldin(\n",
    "        adata: AnnData, group_key: str, latent_key: str\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    if latent_key not in adata.obsm_keys():\n",
    "        return {\"davies_bouldin\": np.nan}\n",
    "    score = davies_bouldin_score(adata.obsm[latent_key], adata.obs[group_key])\n",
    "    return {\"davies_bouldin\": score}\n",
    "\n",
    "\n",
    "def compute_ari_nmi(\n",
    "        adata: AnnData, metric_config: MetricsConfig\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    metrics = {}\n",
    "    for k in metric_config.clustering_range:\n",
    "        for random_seed in range(metric_config.n_random_seeds):\n",
    "            try:\n",
    "                leiden_config = LeidenNClusterConfig(\n",
    "                    random_state=random_seed, clusters=k\n",
    "                )\n",
    "                cluster_algo = LeidenNCluster(leiden_config)\n",
    "                cluster_algo.fit_predict(adata, key_added=metric_config.cluster_key)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                ari = np.nan\n",
    "                nmi = np.nan\n",
    "            else:\n",
    "                ari = compute_ari(adata, metric_config.group_key,\n",
    "                                  metric_config.cluster_key)\n",
    "                nmi = compute_nmi(adata, metric_config.group_key,\n",
    "                                  metric_config.cluster_key)\n",
    "\n",
    "            metrics[f\"ari_{k}_{random_seed}\"] = ari\n",
    "            metrics[f\"nmi_{k}_{random_seed}\"] = nmi\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_neighbors(adata: AnnData, latent_key: str, n_neighbors: int):\n",
    "    if latent_key in adata.obsm.keys():\n",
    "        knn_indices = _get_knn_indices(\n",
    "            adata,\n",
    "            use_rep=latent_key,\n",
    "            n_neighbors=n_neighbors,\n",
    "            calc_knn=True,\n",
    "        )\n",
    "        adata.obsm[\"knn_indices\"] = knn_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal  # pytype: disable=not-supported-yet\n",
    "from typing import Optional\n",
    "\n",
    "import anndata as an  # pytype: disable=import-error\n",
    "import numpy as np\n",
    "import pydantic  # pytype: disable=import-error\n",
    "import scanpy as sc  # pytype: disable=import-error\n",
    "from anndata import AnnData\n",
    "\n",
    "_SupportedMetric = Literal[\n",
    "    \"cityblock\",\n",
    "    \"cosine\",\n",
    "    \"euclidean\",\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"manhattan\",\n",
    "    \"braycurtis\",\n",
    "    \"canberra\",\n",
    "    \"chebyshev\",\n",
    "    \"correlation\",\n",
    "    \"dice\",\n",
    "    \"hamming\",\n",
    "    \"jaccard\",\n",
    "    \"kulsinski\",\n",
    "    \"mahalanobis\",\n",
    "    \"minkowski\",\n",
    "    \"rogerstanimoto\",\n",
    "    \"russellrao\",\n",
    "    \"seuclidean\",\n",
    "    \"sokalmichener\",\n",
    "    \"sokalsneath\",\n",
    "    \"sqeuclidean\",\n",
    "    \"yule\",\n",
    "]\n",
    "\n",
    "\n",
    "class NeighborsGraphConfig(pydantic.BaseModel):\n",
    "    \"\"\"Settings for neighborhood graph computation.\n",
    "    For description, see\n",
    "    https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html\n",
    "    \"\"\"\n",
    "\n",
    "    n_neighbors: int = pydantic.Field(default=15)\n",
    "    n_pcs: Optional[int] = pydantic.Field(default=None)\n",
    "    knn: bool = pydantic.Field(default=True)\n",
    "    # TODO(Pawel): Check whether we can support other methods as well.\n",
    "    method: Literal[\"umap\"] = pydantic.Field(default=\"umap\")\n",
    "    metric: _SupportedMetric = pydantic.Field(default=\"euclidean\")\n",
    "\n",
    "\n",
    "class _LeidenBaseConfig(pydantic.BaseModel):\n",
    "    nngraph: NeighborsGraphConfig = pydantic.Field(default_factory=NeighborsGraphConfig)\n",
    "    random_state: int = pydantic.Field(default=0)\n",
    "    directed: bool = pydantic.Field(default=True)\n",
    "    use_weights: bool = pydantic.Field(default=True)\n",
    "    n_iterations: int = pydantic.Field(default=-1)\n",
    "\n",
    "\n",
    "class BinSearchSettings(pydantic.BaseModel):\n",
    "    start: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3, description=\"The minimal resolution.\"\n",
    "    )\n",
    "    end: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=10.0, description=\"The maximal resolution.\"\n",
    "    )\n",
    "    epsilon: pydantic.PositiveFloat = pydantic.Field(\n",
    "        default=1e-3,\n",
    "        description=\"Controls the maximal number of iterations before throwing lookup \"\n",
    "        \"error.\",\n",
    "    )\n",
    "\n",
    "    @pydantic.validator(\"end\")\n",
    "    def validate_end_greater_than_start(cls, v, values, **kwargs) -> float:\n",
    "        if v <= values[\"start\"]:\n",
    "            raise ValueError(\"In binary search end must be greater than start.\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class LeidenNClusterConfig(_LeidenBaseConfig):\n",
    "    clusters: int = pydantic.Field(\n",
    "        default=5, description=\"The number of clusters to be returned.\"\n",
    "    )\n",
    "    binsearch: BinSearchSettings = pydantic.Field(default_factory=BinSearchSettings)\n",
    "\n",
    "\n",
    "class LeidenNCluster:\n",
    "    def __init__(self, settings: LeidenNClusterConfig) -> None:\n",
    "        self._settings = settings\n",
    "\n",
    "    def fit_predict(self, adata: AnnData, key_added: str) -> np.ndarray:\n",
    "        for offset in [0, 20_000, 30_000, 40_000]:\n",
    "            points = _binary_search_leiden_resolution(\n",
    "                adata,\n",
    "                k=self._settings.clusters,\n",
    "                key_added=key_added,\n",
    "                random_state=self._settings.random_state + offset,\n",
    "                directed=self._settings.directed,\n",
    "                use_weights=self._settings.use_weights,\n",
    "                start=self._settings.binsearch.start,\n",
    "                end=self._settings.binsearch.end,\n",
    "                _epsilon=self._settings.binsearch.epsilon,\n",
    "            )\n",
    "            if points is not None:\n",
    "                break\n",
    "        # In case that for multiple random seeds we didn't find a resolution that\n",
    "        # matches the number of clusters, we raise a ValueError.\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No resolution for the number of clusters {self._settings.clusters}\"\n",
    "                f\" found.\"\n",
    "            )\n",
    "\n",
    "        return points.obs[key_added].astype(int).values\n",
    "\n",
    "\n",
    "def _binary_search_leiden_resolution(\n",
    "    adata: an.AnnData,\n",
    "    k: int,\n",
    "    start: float,\n",
    "    end: float,\n",
    "    key_added: str,\n",
    "    random_state: int,\n",
    "    directed: bool,\n",
    "    use_weights: bool,\n",
    "    _epsilon: float,\n",
    ") -> Optional[an.AnnData]:\n",
    "    \"\"\"Binary search to get the resolution corresponding\n",
    "    to the right k.\"\"\"\n",
    "    # We try the resolution which is in the middle of the interval\n",
    "    res = 0.5 * (start + end)\n",
    "\n",
    "    # Run Leiden clustering\n",
    "    sc.tl.leiden(\n",
    "        adata,\n",
    "        resolution=res,\n",
    "        key_added=key_added,\n",
    "        random_state=random_state,\n",
    "        directed=directed,\n",
    "        use_weights=use_weights,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Get the number of clusters found\n",
    "    selected_k = adata.obs[key_added].nunique()\n",
    "    if selected_k == k:\n",
    "        return adata\n",
    "\n",
    "    # If the start and the end are too close (and there is no point in doing another\n",
    "    # iteration), we raise an error that one can't find the required number of clusters\n",
    "    if abs(end - start) < _epsilon * res:\n",
    "        return None\n",
    "\n",
    "    if selected_k > k:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=start,\n",
    "            end=res,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )\n",
    "    else:\n",
    "        return _binary_search_leiden_resolution(\n",
    "            adata,\n",
    "            k=k,\n",
    "            start=res,\n",
    "            end=end,\n",
    "            key_added=key_added,\n",
    "            random_state=random_state,\n",
    "            directed=directed,\n",
    "            _epsilon=_epsilon,\n",
    "            use_weights=use_weights,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_samples, silhouette_score\n",
    "\n",
    "def silhouette_batch(\n",
    "    adata,\n",
    "    batch_key,\n",
    "    group_key,\n",
    "    latent_key,\n",
    "    metric=\"euclidean\",\n",
    "    return_all=False,\n",
    "    scale=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Batch ASW\n",
    "    Modified average silhouette width (ASW) of batch\n",
    "    This metric measures the silhouette of a given batch.\n",
    "    It assumes that a silhouette width close to 0 represents perfect overlap of the batches, thus the absolute value of\n",
    "    the silhouette width is used to measure how well batches are mixed.\n",
    "    For all cells :math:`i` of a cell type :math:`C_j`, the batch ASW of that cell type is:\n",
    "    .. math::\n",
    "        batch \\\\, ASW_j = \\\\frac{1}{|C_j|} \\\\sum_{i \\\\in C_j} |silhouette(i)|\n",
    "    The final score is the average of the absolute silhouette widths computed per cell type :math:`M`.\n",
    "    .. math::\n",
    "        batch \\\\, ASW = \\\\frac{1}{|M|} \\\\sum_{i \\\\in M} batch \\\\, ASW_j\n",
    "    For a scaled metric (which is the default), the absolute ASW per group is subtracted from 1 before averaging, so that\n",
    "    0 indicates suboptimal label representation and 1 indicates optimal label representation.\n",
    "    .. math::\n",
    "        batch \\\\, ASW_j = \\\\frac{1}{|C_j|} \\\\sum_{i \\\\in C_j} 1 - |silhouette(i)|\n",
    "    :param batch_key: batch labels to be compared against\n",
    "    :param group_key: group labels to be subset by e.g. cell type\n",
    "    :param embed: name of column in adata.obsm\n",
    "    :param metric: see sklearn silhouette score\n",
    "    :param scale: if True, scale between 0 and 1\n",
    "    :param return_all: if True, return all silhouette scores and label means\n",
    "        default False: return average width silhouette (ASW)\n",
    "    :param verbose: print silhouette score per group\n",
    "    :return:\n",
    "        Batch ASW  (always)\n",
    "        Mean silhouette per group in pd.DataFrame (additionally, if return_all=True)\n",
    "        Absolute silhouette scores per group label (additionally, if return_all=True)\n",
    "    \"\"\"\n",
    "    if latent_key not in adata.obsm.keys():\n",
    "        print(adata.obsm.keys())\n",
    "        raise KeyError(f\"{latent_key} not in obsm\")\n",
    "\n",
    "    sil_per_label = []\n",
    "    for group in adata.obs[group_key].unique():\n",
    "        adata_group = adata[adata.obs[group_key] == group]\n",
    "        n_batches = adata_group.obs[batch_key].nunique()\n",
    "\n",
    "        if (n_batches == 1) or (n_batches == adata_group.shape[0]):\n",
    "            continue\n",
    "\n",
    "        sil = silhouette_samples(\n",
    "            adata_group.obsm[latent_key], adata_group.obs[batch_key], metric=metric\n",
    "        )\n",
    "\n",
    "        # take only absolute value\n",
    "        sil = [abs(i) for i in sil]\n",
    "\n",
    "        if scale:\n",
    "            # scale s.t. highest number is optimal\n",
    "            sil = [1 - i for i in sil]\n",
    "\n",
    "        sil_per_label.extend([(group, score) for score in sil])\n",
    "\n",
    "    sil_df = pd.DataFrame.from_records(\n",
    "        sil_per_label, columns=[\"group\", \"silhouette_score\"]\n",
    "    )\n",
    "\n",
    "    if len(sil_per_label) == 0:\n",
    "        sil_means = np.nan\n",
    "        asw = np.nan\n",
    "    else:\n",
    "        sil_means = sil_df.groupby(\"group\").mean()\n",
    "        asw = sil_means[\"silhouette_score\"].mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"mean silhouette per group: {sil_means}\")\n",
    "\n",
    "    if return_all:\n",
    "        return asw, sil_means, sil_df\n",
    "\n",
    "    return {\"asw_batch_score\":asw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbet_for_different_cell_type(adata, latent_key, batch_key, label_key):\n",
    "    k = len(adata.obs[label_key].unique())\n",
    "    kBET_scores = {\"cell_type\": [], \"kBET\": []}\n",
    "    for i in range(k):\n",
    "        kBET_scores[\"cell_type\"].append(adata.obs[label_key].unique()[i])\n",
    "        kbet_score = kbet(adata[adata.obs[label_key].isin(adata.obs[label_key].unique()[[i]])], latent_key = latent_key, batch_key = batch_key, label_key = label_key)\n",
    "        kBET_scores[\"kBET\"].append(kbet_score)\n",
    "    return kBET_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_kbet_for_different_cell_type(adata, batch_key, label_key, latent_key, print_UMAP):\n",
    "    sec_model_imm = second_VAE(adata, latent_id_imm, n_layers = 5, n_hidden = 512, n_latent = 10, lr = 10e-3)\n",
    "    plot_reconstruction_loss_and_elbo(sec_model_imm)\n",
    "    get_latent_secUMAP(sec_model_imm, adata, batch_key, label_key, latent_key, print_UMAP)\n",
    "    kbet_collection = kbet_for_different_cell_type(adata, latent_key = latent_key, batch_key = batch_key, label_key = latent_key)\n",
    "    print(kbet_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbet_rni_asw(adata, latent_key, batch_key, label_key, group_key, max_clusters):\n",
    "    bdata = adata.copy()\n",
    "    ari_score_collection = []\n",
    "    k = np.linspace(2, max_clusters, max_clusters-1)\n",
    "    for i in k:\n",
    "        cdata = _binary_search_leiden_resolution(bdata, k = int(i), start = 0.1, end = 0.9, key_added ='final_annotation', random_state = 0, directed = False, \n",
    "        use_weights = False, _epsilon = 1e-3)\n",
    "        if cdata is None:\n",
    "            ari_score_collection.append(0)\n",
    "            continue\n",
    "        adata.obs['cluster_{}'.format(int(i))] = cdata.obs['final_annotation']\n",
    "        ari_score_collection.append(compute_ari(adata, group_key = group_key, cluster_key = 'cluster_{}'.format(int(i))))\n",
    "\n",
    "\n",
    "    # Note, all keys should come from the columns in adata.obs\n",
    "    ari_score = {f\"maximum ARI_score with {int(k[np.argmax(ari_score_collection)])} clusters\": np.max(ari_score_collection)}\n",
    "    sc.pl.umap(adata, color = ['cluster_{}'.format(int(k[np.argmax(ari_score_collection)]))])\n",
    "    kbet_score = kbet(adata, latent_key=latent_key, batch_key=batch_key, label_key=label_key)\n",
    "    asw_score = compute_asw(adata, group_key = group_key, latent_key = latent_key)\n",
    "    asw_batch_score = silhouette_batch(adata, batch_key = batch_key, group_key= group_key, latent_key= latent_key)\n",
    "\n",
    "    return [kbet_score, ari_score, asw_score, asw_batch_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_scale(dataset):\n",
    "    if np.max(dataset) - np.min(dataset) != 0:\n",
    "        return (dataset - np.min(dataset)) / (np.max(dataset) - np.min(dataset))\n",
    "    if np.max(dataset) - np.min(dataset) == 0:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_list_generator(layers, num_latent, lr):\n",
    "    grid_search_list = []\n",
    "    for i in range(len(layers)):\n",
    "        for j in range(len(num_latent)):\n",
    "            for k in range(len(lr)):\n",
    "                grid_search_list.append([layers[i], num_latent[j], lr[k]])\n",
    "\n",
    "    return grid_search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_list = grid_search_list_generator(layers = [1,3,5], \n",
    "num_latent = [5,10,15], \n",
    "lr =[1e-3,1e-4,1e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = []\n",
    "for i in grid_search_list:\n",
    "    col_name.append(\"VAE layers of %s, num latents of %s, lr of %s\" % (i[0],i[1],i[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(adata, latent_info, batch_key, label_key, group_key, max_clusters, grid_search_list):\n",
    "    # After data_spliting_del, train_model_firstVAE and fetch_batch_information\n",
    "\n",
    "\n",
    "    ari_collection = []\n",
    "    asw_batch_collection = []\n",
    "    kbet_collection = [] \n",
    "    asw_collection = []\n",
    "    for i in grid_search_list:\n",
    "        model = second_VAE(adata, latent_info = latent_info, n_layers = i[0], n_hidden = 512, n_latent = i[1], lr = i[2])\n",
    "        print(\"VAE layers of %s, num latents of %s, lr of %s\" % (i[0],i[1],i[2]))\n",
    "        latent_key = get_latent_secUMAP(model, adata, batch_key, label_key, added_latent_key = 'X_secVAE_{}'.format(i), print_UMAP = True)\n",
    "        score_collection = kbet_rni_asw(adata, latent_key = latent_key, batch_key = batch_key, label_key = label_key, group_key = group_key, max_clusters = max_clusters)\n",
    "        for i in score_collection[1].values():\n",
    "            ari_collection.append(i)\n",
    "        for i in score_collection[3].values():\n",
    "            asw_batch_collection.append(i)\n",
    "        for i in score_collection[0].values():\n",
    "            kbet_collection.append(i)\n",
    "        for i in score_collection[2].values():\n",
    "            asw_collection.append(i)\n",
    "\n",
    "    ari_collection_mn = max_min_scale(ari_collection)\n",
    "    asw_batch_collection_mn = max_min_scale(asw_batch_collection)\n",
    "    kbet_collection_mn = max_min_scale(kbet_collection)\n",
    "    asw_collection_mn = max_min_scale(asw_collection)\n",
    "\n",
    "    bio_score_collection = [] \n",
    "    batch_score_collection = [] \n",
    "    overall_score_collection = []\n",
    "    for i in range(len(grid_search_list)):\n",
    "        bio_score = np.mean((ari_collection_mn[i], asw_collection_mn[i]))\n",
    "        bio_score_collection.append(bio_score)\n",
    "        batch_score = np.mean((kbet_collection_mn[i], asw_batch_collection_mn[i]))\n",
    "        batch_score_collection.append(batch_score)\n",
    "        overall_score_collection.append(0.6 * bio_score + 0.4 * batch_score)\n",
    "    return [ari_collection, asw_collection, kbet_collection, asw_batch_collection, bio_score_collection, batch_score_collection, overall_score_collection]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = sc.read_h5ad(\"Immune_ALL_human.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata, latent_id_imm = first_VAE(idata, 'batch', 'final_annotation', 'counts', [0,2,3,4,6,7,8,9,10,11,12,13,14,15], [1,5], True, \"X_adverVAE_1\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = hyperparameter_tuning(mdata, latent_info = latent_id_imm, batch_key= 'batch', label_key= 'final_annotation', group_key= 'final_annotation', max_clusters = 8, grid_search_list = grid_search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_scorelist_into_df(scorelist, variable_name, store, csv_file_name):\n",
    "    score_pd = pd.DataFrame(scorelist, index = [\"ari\", \"asw_cell\", \"kbet\", \"asw_batch\",\"bio_score\", \"batch_score\", \"overall_score\"], columns = variable_name)\n",
    "    if store:\n",
    "        score_pd.to_csv(csv_file_name)\n",
    "    return score_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pd = convert_scorelist_into_df(score, col_name, True,'grid_NO_adver_train_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldata = sc.read_h5ad(\"Lung_atlas_public.h5ad\")\n",
    "mmldata, latent_id_lung = first_VAE(ldata, 'batch', 'cell_type', 'counts', [1,3,4,5,6,7,8,9,10,11,12,13,14,15,16], [0,2], False, \"X_adverVAE_1\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lung = hyperparameter_tuning(mmldata, latent_info = latent_id_lung, batch_key= 'batch', label_key= 'cell_type', group_key= 'cell_type', max_clusters = 8, grid_search_list = grid_search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lung_pd = convert_scorelist_into_df(score_lung, col_name, True,'grid_NO_adver_train_lung.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_lung_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = sc.read_h5ad(\"human_pancreas_norm_complexBatch.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmpdata, latent_id_pancreas = first_VAE(pdata, 'tech', 'celltype', 'counts', [1,3,4,5,6,7,8,9,10,11,12,13], [0,2], False, \"X_adverVAE_1\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pancreas = hyperparameter_tuning(mmpdata, latent_info = latent_id_pancreas, batch_key= 'tech', label_key= 'celltype', group_key= 'celltype', max_clusters = 8, grid_search_list = grid_search_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pancreas_pd = convert_scorelist_into_df(score_pancreas, col_name, True, 'grid_No_adver_train_pancreas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_scvi2",
   "language": "python",
   "name": "lab_scvi2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1fcf5ce4f39e09cd072220dda92b1b07d2539b28e330b861d32713f27d750be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
